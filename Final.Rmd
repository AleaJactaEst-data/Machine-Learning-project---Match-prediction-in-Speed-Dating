---
title: "Dating_data : la data qui vous date"
author: "Florian JACTA, Hugo MICCINILLI, Theo DI PIAZZA, Aboa BOUADOU, Linh NGUYEN"
date: "26/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart)
library(xgboost)
library(ROSE)
library(MLmetrics)
library(caret)
library(doSNOW)
library(dplyr)
library(stringr)
library(rcompanion)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(cramer)
library(rcompanion)
library(tensorflow)
library(glmnet)
library(pROC)
#install_tensorflow() # si jamais vous n'avez jamais utilisé tensorflow, il faut l'installer avec cette commande

library(keras)
```


# Fonctions nécessaire pour le bon déroulement du projet

```{r}

optimisationNeuroneDeeplearning = function(gamma_min, gamma_max, gamma_pas, epoch, xtrain, xtest,ytrain, ytest){
  sol = as.data.frame(c(0,0,0))
  
  for( i in seq(gamma_min, gamma_max, gamma_pas)){
    
    focal_loss=function(y_TRUE, y_pred){
      gamma = i 
      print("gamma")
      pt = y_pred * y_TRUE + (1-y_pred) * (1-y_TRUE)
      print("pt")
      pt = k_clip(pt, 0, 1)
      print("pt2")
      CE = -k_log(pt+k_epsilon())
      print("CE")
      FL = k_pow(1-pt, gamma) * CE
      print("FL")
      loss = k_sum(FL, axis=1)
      print(loss)
      return(loss)
    }
    
    model <- keras_model_sequential()
    model %>% 
      layer_dense(units = round(ncol), input_shape = c(ncol), activation = "sigmoid") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = round(ncol*0.7),  activation = "sigmoid") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = round(ncol*0.5),  activation = "sigmoid") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = round(ncol*0.3), activation ="relu") %>%
      layer_dropout(0.3)  %>%
      
      layer_dense(units = 1, activation = "sigmoid")
    

    
    model %>% compile(
      loss = focal_loss,
      optimizer = 'adam',
      metrics = f1_m
    )
    
    history <- model %>% fit(
      xtrain,  ytrain, 
      batch_size =0.05,epochs = epoch,
      validation_split = 0.15,
      view_metrics = FALSE
    );
    
    
    #prédiction sur l'échantillon test
    predSimple <- model %>% predict_classes(xtest)
    #print(table(predSimple))
    acc = sum(predSimple == ytest)/length(ytest)
    if(sum(ytest) != 0){
      sensitivity = sum(predSimple == ytest && ytest == 1)/sum(ytest)
    }else{
      sensitivity = 0
    }
    
    sol = cbind(sol, c(i,acc,sensitivity))
    print(c(i,acc,sensitivity))
    
    remove(model, history, predSimple, acc, sensitivity, focal_loss)
  }
  
  return(sol)
  
}


f1 <- function (data, lev = NULL, model = NULL) {
  precision <- Precision(data$pred, data$obs,positive ="1")
  recall  <- Recall(data$pred, data$obs,positive ="1")
  f1_val <- F1_Score(data$pred, data$obs,positive ="1")
  resu=c("precision"=precision,"rappel"=recall,"F1"=f1_val)
  resu
}


```

# Présentation de la base

Pour ce projet, nous avons voulu aborder un thème qui nous tient à coeur : l'amour. Pour cela, nous avons récupéré les données issues d'une étude réalisée entre 2002 et 2004 par la Columbia Business School sur des individus hétérosexuels. Cette étude avait notamment pour objectif d'étudier les facteurs pouvant influencer le match (fait que deux individus veuillent sortir ensemble).

## Présentation de la base

Chaque ligne de ce jeu données représente une rencontre entre un individu principal et un individu secondaire. 
Pendant les événements, les participants ont un «premier rendez-vous» de quatre minutes avec tous les autres participants du sexe opposé. À la fin des quatre minutes, on a demandé aux participants s'ils aimeraient revoir leur rendez-vous. Un match est donc une variable binaire indiquant si les deux partenaires souhaitent se revoir. On leur a également demandé d'évaluer leur partenaire de dating sur six attributs: l'attractivité, la sincérité, l'intelligence, le plaisir, l'ambition et les intérêts partagés.


L'ensemble de données comprend également des données de questionnaire recueillies auprès des participants à différents moments du processus. Ces domaines comprennent: la démographie, les habitudes de rencontres, la perception de soi à travers les attributs clés, les croyances sur ce que les autres trouvent précieux chez un partenaire et les informations sur le mode de vie. 


De plus, ce questionnaire a été envoyé sous plusieurs vagues aux participant pour voir si'il avait une évolution de leur perception d'eux, de leurs attentes....

Notre objectif dans ce projet est de nous mettre à la place d'une start-up voulant créer une application de rencontre. Pour cela nous allons utiliser les données de cette études afin que notre application mette en relation des personnes qui ont le plus de chance de se matcher. 

# Preprocessing

Dans un premier temps il est nécessaire de nettoyer notre jeu de données et de le préparer pour répondre à notre problématique

## Gestion des valeurs 
Avec les fonctions ci-dessous, nous gérons les problèmes d'encodages de certaines variables et de formats.
```{r}
# Gestion Variables

# Undergra
gestionUndergra = function(df){
  df$undergra = as.character(df$undergra)
  df$undergra[which(df$undergra!="")] = 1
  df$undergra[which(df$undergra=="")] = 0
  df$undergra = factor(df$undergra)
  
  return(df)
}

# Career
gestionCareer = function(df){
  #Si career : lawyer alors career_c : 1
  df = df %>% mutate(career_c = ifelse(career=="lawyer" | career=="law", 1, career_c))
  
  #Si career : Economist alors career_c : 7 (with Finance...)
  df = df %>% mutate(career_c = ifelse(career=="Economist", 7, career_c))
  
  #Si career : tech professional  alors career_c : 7 (with Finance...)
  df = df %>% mutate(career_c = ifelse(career=="tech professional", 5, career_c))
  
  #Suppression des autres individus : quasiment toutes leurs variables sont nulles
  to_delete = df$career_c %>% is.na %>% which
  df = df[!seq_len(nrow(df)) %in% to_delete, ]
  
  df %>% dplyr::select(c("career_c","career"))
  
  return(df)
}

# Income
gestionIncome = function(df){
  income_ = as.character(factor(df$income)) # on convertit en character les valeurs
  
  for(i in 1:dim(df)[1]){
    string = income_[i] # on prend notre character
    id  = nchar(string) - 6 # l'indice où est la virgule
    if(id>0 & !is.na(id)){
      str_sub(string, id, id) <- "" # on remplace la virgule par rien
    }
    income_[i] = string
  }
  
  df$income = as.numeric(income_) # on change toutes les valeurs comme il faut dans le dataframe
  
  return(df)
}
```

## #Homogénéisation systeme de points
L'étude a été faite sur plusieurs vagues et durant la notation des attributs (sincérité, intelligence...), deux systèmes de notation ont été utilisés pour un même sujet. C'est pourquoi, afin d’homogénéiser le système de points ainsi que de répondre à notre objectif, nous avons harmonisé le système de points puis effectué une moyenne sur toutes les vagues pour chaque attribut concernant le meme sujet (perception de soie, attentes de son partenaire..)  
```{r}
agregationVariable = function(df){
  df2=df
  # CODE POUR GERER LE REGROUPEMENT DE VARIABLE
  attribut=c("attr","sinc","intel","fun","amb","shar")
  time=c(as.character(1:3),"s")
  catg=c(1:5,7)
  
  var_gen=outer(attribut,outer(catg,paste0("_",time),paste0),paste0)
  var_avg=outer(attribut,outer(catg,"_avg",paste0),paste0)
  
  #Etape 1 : harmoniser le systeme de point
  for(k in c(1,2,4)){#theme qui ont besoin d'être harmonisé
    for(i in 1:4){
      if(all(var_gen[,k,i]%in%names(df2))){
        df2[,var_gen[,k,i]]=sapply(var_gen[,k,i],function(v) 100*df2[,v]/apply(df2[,var_gen[,k,i]],1,sum,na.rm = T)) 
      }
    }
  }
  
  #ETAPE2 : regroupement
  for(k in 1:6){#catg/
    for(i in 1:6){#attribut
      v=var_gen[i,k,][which(var_gen[i,k,]%in%names(df2))]
      df2[,var_avg[i,k,1]]= apply(df2[,v],1,mean,na.rm = T)
    }
  }
  
  #ETAPE3 : Supression des anciennes variables
  v=as.vector(var_gen)
  v=v[which(v%in%names(df2))]
  df2=df2[,-which(names(df2)%in%v)]
  
  remove(attribut, catg, i, k, time, v, var_avg, var_gen)
  
  return(df2)
}
```

## Aggregation
Certaines variables qualitatives disposaient d'un grand nombre de madalité (parfois 17) c'est pourquoi pour certaines d'entres elles, nous avons créé une nouvelle variable qui les reprend en aggrégeant certaines de leurs modalités.
```{r}
aggregateCarrer=function(df_individu){
  #carrer_cd
  law=c(1)
  science=c(4,5,12)
  huma=c(3,9,11,13,16)
  art=c(6,17)
  business=c(7,8)
  chercheur=c(2)
  sportif=c(14)
  autre=c(10,15)
  
  df_individu$career_c2=8
  df_individu$career_c2[df_individu$career_c%in%law]=1 #inutile mais bon
  df_individu$career_c2[df_individu$career_c%in%science]=2
  df_individu$career_c2[df_individu$career_c%in%huma]=3
  df_individu$career_c2[df_individu$career_c%in%art]=4
  df_individu$career_c2[df_individu$career_c%in%business]=5
  df_individu$career_c2[df_individu$career_c%in%chercheur]=6
  df_individu$career_c2[df_individu$career_c%in%sportif]=7
  
  return(df_individu)
}

aggregateField=function(df_individu){
  #field_cd
  law=c(1)
  science=c(2,4,5,10)
  huma=c(3,6,7,16,13,9)
  art=c(14,15,17)
  business=c(8)
  autre=c(12,17)
  
  df_individu$field_cd2=6
  df_individu$field_cd2[df_individu$field_cd%in%law]=1 #inutile mais bon
  df_individu$field_cd2[df_individu$field_cd%in%science]=2
  df_individu$field_cd2[df_individu$field_cd%in%huma]=3
  df_individu$field_cd2[df_individu$field_cd%in%art]=4
  df_individu$field_cd2[df_individu$field_cd%in%business]=5
  
  return(df_individu)
}

aggregateActivitie=function(df_individu){
  #activite1
  df_individu$act_sports=apply(df_individu,1,function(x) mean(x[c("sports","tvsports","exercise","hiking","yoga","clubbing")],na.rm=T))
  df_individu$act_art=apply(df_individu,1,function(x) mean(x[c("museums","art","reading","theater","movies","concerts","music")],na.rm=T))
  df_individu$act_autre=apply(df_individu,1,function(x) mean(x[c("dining","gaming","tv","shopping")],na.rm=T))
  
  #activite2
  df_individu$act_casanier=apply(df_individu,1,function(x) mean(x[c("tvsports","gaming","tv","yoga","art","reading","music","movies")],na.rm=T))
  df_individu$act_sortie=apply(df_individu,1,function(x) mean(x[c("sports","museums","theater","concerts","exercise","hiking","clubbing","dining","shopping")],na.rm=T))
  
  return(df_individu)
}
```

## Gestion des NA
```{r}
# Remplissage par la moyenne
gestionDesNASystematiques = function(df_individu){
  
  for(var in c("age","field_cd", "date", "amb2_avg", "shar2_avg")){
    id = which(is.na(df_individu[,var]))
    if(length(id)>0){
      df_individu = df_individu[-id,]
    }
  }
  
  for(var in c("attr4_avg", "sinc4_avg", "intel4_avg", "fun4_avg", "amb4_avg", "shar4_avg", "attr5_avg", "sinc5_avg", "intel5_avg", "fun5_avg", "amb5_avg")){
    id = which(is.na(df_individu[,var]))
    m = mean(df_individu[-id,var])
    df_individu[id,var] = m
  }
  
  return(df_individu)
}
```

## Creation des couples
Cette fonction va nous permettre de créer notre base couple finale en faisant en sorte qu'un couple n'apparaissent qu'une fois et que les informations des deux partenaires soient présents pour chaque ligne. Les variables se terminant par .x désignent celles relatives aux femmes et par .y celles relaives aux hommes
```{r}
creationDf_Couple = function(df,df_individu){
  df_couple_id = df[,c('iid','pid', 'match',"gender")]
  df_couple_id=df_couple_id[order(df_couple_id$gender),]
  
  coupleDated = c()
  ligneAenlever = c()
  
  for(i in 1:dim(df_couple_id)[1]){
    iid = df_couple_id$iid[i]
    pid = df_couple_id$pid[i]
    
    strCouple = paste(as.character(iid),as.character(pid))
    strCoupleInv = paste(as.character(pid),as.character(iid))
    
    if(!(strCoupleInv %in% coupleDated)){
      coupleDated = c(coupleDated,strCouple)
    }
    else{
      ligneAenlever = c(ligneAenlever,i)
    }
  }
  df_couple_id= df_couple_id[-ligneAenlever,]%>%select(-c("gender"))
  
  df_couple = merge(merge(df_couple_id, df_individu, by = 'iid'), df_individu, by.x = 'pid', by.y = 'iid')
  
  
  return(df_couple)
}
```

## Fonction du prepocessing
Enfin nous avons défini une fonction qui fait appel à celles dont nous venons d'évoquer plus haut.
```{r}
importation=function(){
  pb=txtProgressBar(style = 3,width = 50)
  
  setTxtProgressBar(pb, 0.14)
  df = read.csv("DataBase/Speed Dating Data.csv")

  df = df %>% select(-c("positin1","position", "expnum","exphappy"))
  
  #Nettoyage
  setTxtProgressBar(pb, 0.28)
  df = df%>%gestionUndergra()%>%gestionCareer()%>%gestionIncome()

  
  #Homogénisation systeme de points + aggregation
  setTxtProgressBar(pb, 0.42)
  df=agregationVariable(df)
 
  
  # Individus
  setTxtProgressBar(pb, 0.6)
  df_individu = df[!duplicated(df[,c('iid')]),]
  
  listeASupp = c('id','idg','round','wave','order','condtn','int_corr','partner','pid', 'match','samerace','age_o','race_o', 'pf_o_sin','pf_o_int','pf_o_fun','pf_o_amb','pf_o_sha','dec_o', 'sinc_o','attr_o', 'intel_o','fun_o','amb_o','shar_o','like_o','prob_o','met_o','match_es','pf_o_att','dec','attr','sinc','intel','fun','fun','amb','shar','like','prob','met','you_call','them_cal','length', 'date_3', 'numdat_3', 'num_in_3','numdat_2', 'satis_2','positin1','position', 'expnum','attr7_avg','sinc7_avg','intel7_avg','fun7_avg','amb7_avg','shar7_avg','shar3_avg','shar5_avg')
  
  listeASupp=c(listeASupp,c("field","undergra","mn_sat","tuition","from","zipcode","career","income"))
  
  df_individu = df_individu[,-which(names(df) %in% listeASupp)]
  
  
  # Aggregation de variable
  setTxtProgressBar(pb, 0.7)
  df_individu=df_individu%>%aggregateField()%>%aggregateCarrer()%>%aggregateActivitie()
  
  
  # GESTION DES NA
  setTxtProgressBar(pb, 0.8)
  df_individu=gestionDesNASystematiques(df_individu)
  
  # Creation df_couple
  setTxtProgressBar(pb, 0.9)
  df_couple=creationDf_Couple(df,df_individu)
  
  setTxtProgressBar(pb, 1)
  return(df_couple%>%select(-c("pid","iid")))
  
}
```

# Feature engineering
Une fois que nous avons eu notre base couple, nous avons fait quelques créations de nouvelles variables.

## Variables quantitatives
Dans cette partie, nous créons plusieurs variables afin de mettre en lien les variables des hommes et des femmes comme des variables distances qui calculent la distance/norme entre les activités des hommes et des femmes.
```{r}
creation_avg_ect=function(df_couple2){
  var_num=c("act_sortie","act_casanier","act_autre","act_art","act_sports","age","imprace","imprelig",
            "sports", "tvsports","exercise","dining","museums","art","hiking","gaming","clubbing","reading","tv","theater","movies","concerts","music","shopping","yoga",
            paste0("attr",1:5,"_avg"),paste0("sinc",1:5,"_avg"),paste0("intel",1:5,"_avg"),paste0("fun",1:5,"_avg"),paste0("amb",1:5,"_avg"),paste0("shar",c(1,2,4),"_avg")
  )
  
  #Calcule moy et ect
  for(v in var_num){
    df_couple2[,paste0(v,"_moy")]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,mean,na.rm=F)
    df_couple2[,paste0(v,"_ect")]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,function(x) abs(x[1]-x[2]))
  }
  
  # Calcule de distance
  #activite
  activite=c("sports", "tvsports","exercise","dining","museums","art","hiking","gaming","clubbing","reading","tv","theater","movies","concerts","music","shopping","yoga")
  df_couple2$act_dist=apply(df_couple2[,paste0(activite,"_ect")]^2,1,function(x) sqrt(sum(x)))
  #activite1
  df_couple2$act1_dist=apply(df_couple2[,paste0(c("act_sortie","act_casanier"),"_ect")]^2,1,function(x) sqrt(sum(x)))
  #activite2
  df_couple2$act2_dist=apply(df_couple2[,paste0(c("act_autre","act_art","act_sports"),"_ect")]^2,1,function(x) sqrt(sum(x)))
  
  #Attribut
  attribut=c("attr","sinc","intel","fun","amb","shar")
  #Ce que tu cherches chez l'autre
  df_couple2$attribut1_dist=apply(df_couple2[,paste0(attribut,"1_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses que les gens de sexe opposé cherchent chez l'autre
  df_couple2$attribut2_dist=apply(df_couple2[,paste0(attribut,"2_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses de toi
  df_couple2$attribut3_dist=apply(df_couple2[,paste0(attribut[-6],"3_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses que ton sexe cherche chez l'autre
  df_couple2$attribut4_dist=apply(df_couple2[,paste0(attribut,"4_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que les autres pensent de toi
  df_couple2$attribut5_dist=apply(df_couple2[,paste0(attribut[-6],"5_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  
  df_couple2$correspondance1_dist=apply((df_couple2[,paste0(attribut[-6],"1_avg.x")]-df_couple2[,paste0(attribut[-6],"3_avg.y")])^2,
                                        1,function(x) sqrt(sum(x)))
  
  df_couple2$correspondance2_dist=apply((df_couple2[,paste0(attribut[-6],"1_avg.y")]-df_couple2[,paste0(attribut[-6],"3_avg.x")])^2,
                                        1,function(x) sqrt(sum(x)))
  
  
  var_num=unlist(lapply(var_num,function(x) paste0(x,c(".x",".y","_moy","_ect"))))
  var_num=c(var_num,"act_dist","act1_dist","act2_dist","attribut1_dist","attribut2_dist","attribut3_dist","attribut4_dist","attribut5_dist","correspondance1_dist","correspondance2_dist")
  
  
  return(list("df"=df_couple2, "var_num" =  var_num))
}

```

## Variables qualitatives
Dans cette partie,nous créons principalement des variables indicatrices et de comptages.
```{r}
creation_same_ind=function(df_couple2){
  
  var_qual=c("field_cd","race", "goal","date","go_out","career_c")
  var_qual2=unlist(lapply(var_qual,function(x) paste0(x,c(".x",".y"))))
  
  
  #Variable qualitative Solution1: varaible somme de modalite + indicatrice same modalitee
  for( v in var_qual){
    v_modalite=unique(c(df_couple2[,paste0(v,".x")],df_couple2[,paste0(v,".y")]))
    #indicatrice same
    var_qual2=c(var_qual2,paste0("same_",v))
    df_couple2[,paste0("same_",v)]= as.numeric(df_couple2[,paste0(v,".x")]==df_couple2[,paste0(v,".y")])
    #comptage
    for(mod in v_modalite){
      var_qual2=c(var_qual2,paste0(v,"_",mod))
      df_couple2[,paste0(v,"_",mod)]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,function(x) sum(c(x[1]==mod,x[2]==mod),na.rm = T))
    }
  }
  for(var in c("match",var_qual2)){
    df_couple2[,var] = as.factor(df_couple2[,var])
  }
  
  return(list("df"=df_couple2, "var_qual" =  var_qual2))
}
```


```{r }

feature_eng=function(){
  df_couple2=importation()
  
  # Var numerique -----------------------------------------------------------
  
  resu=creation_avg_ect(df_couple2)
  df_couple2=resu$df
  var_num = resu$var_num
    
  # Var qualitative ---------------------------------------------------------
  
  resu=creation_same_ind(df_couple2)
  df_couple2=resu$df
  var_qual = resu$var_qual

  
  return(list("df"=df_couple2,"var_num"=var_num,"var_qual"=var_qual))
}

```


# Création de la base de donnée finale

feature_eng() permet de prendre notre base de donnée corrigée, transformée, vidée de ces Na, etc... Elle change des formats, met les variables en numérique et en factor, crée des variables pour diviser les variables factorielles avec beaucoup de modalités en plusieurs variables avec moins de modalités. Elle permet aussi d'enlever les symétries du modèle (une paire apparait deux fois), elle crée des variables de distance et de moyenne des variables numériques, enlève les informations que nous ne pouvons pas savoir à priori (avant le date) etc...



feature_selection() va permettre de donner toutes les varaiables non significatifs à partir d'un seuil. Les variables numériques et factorielles ne sont pas testées de la même façon.

creationDesData() va permettre de créer notre base finale avec aussi les bases de test et d'apprentissage.

```{r}
feature_selection=function(df_couple2,var_num,var_qual2,seuil=0.05,nb_var_max=Inf){
  if(length(seuil)==1){
    seuil=c(seuil,seuil)
  }
  if(length(nb_var_max)==1){
    nb_var_max=c(nb_var_max,nb_var_max)
  }
  #Numerique
  var_num_sign=sapply(var_num,function(v)  t.test(df_couple2[,v]~df_couple2$match)$p.value)
  var_num_sign=var_num_sign[order(var_num_sign)]
  var_num_sign=var_num_sign[var_num_sign<=seuil[1]]
  var_num_sign=var_num_sign[1:min(nb_var_max[1],length(var_num_sign))]
  barplot(var_num_sign,main = "Test student variable numerique")
  
  
  #Qualitative
  #var_qual_sign=sapply(var_qual2,function(v) {cramerV(table(df_couple2[,c("match",v)]))})
  
  var_qual_sign=sapply(var_qual2,function(v)  chisq.test(df_couple2[,v],df_couple2$match)$p.value)
  var_qual_sign=var_qual_sign[order(var_qual_sign)]
  var_qual_sign=var_qual_sign[var_qual_sign<=seuil[2]]
  var_qual_sign=var_qual_sign[1:min(nb_var_max[2],length(var_qual_sign))]
  
  return(c("match",names(c(var_qual_sign,var_num_sign))))
}
# creation de df_mod, dapp et dtest
# noFactor sert pour le réseau de neurone, un réseaud de neurone ne peut pas prendre de factor
creationDesData = function(df_couple2, varSignificatifs = FALSE, varSupp = FALSE, noFactor = FALSE,p_split=0.7){
  if(length(varSignificatifs) > 1){
    df_mod = df_couple2[,which(names(df_couple2) %in% varSignificatifs)]
  }
  
  if(length(varSupp) > 1){
    if(length(which(names(df_mod) %in% varSupp))>0){
      df_mod =df_mod[,-which(names(df_mod) %in% varSupp)]
    }
  }
  
  if(noFactor){
    for(var in varFactor){
      if(var %in% names(df_mod)){
        df_mod[,var] = as.numeric(df_mod[,var]) -1
      }
    }
    remove(var)
    
    df_mod[,which(names(df_mod) %in% varFactor)] = df_mod[,which(names(df_mod) %in% varFactor)] -1
    df_mod$match = df_mod$match + 1
  }
  
  
  set.seed(1234)
  perm <- createDataPartition(df_mod$match, p = p_split, list = FALSE)
  dapp <- df_mod[perm,]
  dtest <- df_mod[-perm,]
  
  
  return(list(df_mod = df_mod,dapp = dapp,dtest = dtest))
  
}
```


```{r}
df_mod = feature_eng()
#var significatif 
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual) # on peut choisir le seuil pour prendre les meilleurs # variables significatifs

# Creation des bases de test et d'apprentissage

resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest
```

# Statistique descriptive et visualisation

Dans cette partie il s'agit d'utiliser des outils de visualisation et de statistique pour découvrir notre base de données finale, en particulier les corrélations qu'il pourrait y avoir entre les variables.
Pour cela, nous étudierons les corrélations entre les variables quantitatives, et les éventuelles corrélations qu'il pourrait y avoir entre les variables quantitatives/qualitatives avec la variable à expliquer. Grâce au rapport de corrélation, au test de Student et le V de Cramer.


```{r}
#Selection des variables numeriques
df_num = df_mod %>% select(where(is.numeric))
#Les variables numeriques concernant les avis des femmes
df_num_girl = df_num[,names(df_num)[sapply(names(df_num),function(x) str_detect(x,"\\.x"))]]
  #Les variables numeriques concernant les avis des hommes
df_num_boy = df_num[,names(df_num)[sapply(names(df_num),function(x) str_detect(x,"\\.y"))]]
df_num_moy = df_num[,names(df_num)[sapply(names(df_num),function(x) !str_detect(x,"\\.x") & !str_detect(x, "\\.y"))]]

#Calcul des matrices de correlation
M_girl <- cor(df_num_girl)
M_boy <- cor(df_num_boy)
M_moy <- cor(df_num_moy)

#Affichage des matrices de correlation
corrplot(M_girl, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"))
corrplot(M_boy, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"))
corrplot(M_moy, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"), tl.cex=0.6)
```

Pour les femmes:
On observe une forte corrélation entre la partitipation d'une femme aux activités artistiques et sa participation à des activités telles que des musées ou concerts.

Pour les hommes :
attr4_avg.y : Note qui indique si l'individu pense que les hommes cherchent la beauté chez leur partenaire.
amb4_avg.y : Note qui indique si l'individu pense que les hommes cherchent l'ambition chez leur partenaire.
sinc4_avg.y : Note qui indique si l'individu pense que les hommes cherchent la sincérité chez leur partenaire.

On observe une corrélation négative entre attr4_avg.y à amb4_avg.y, sinc4_avg.y.
Lorsqu'une personne pense que les hommes accordent de l'importance à la beauté, elle considère généralement qu'ils ne s'intéressent pas à l'ambition ou à la sincérité de leur partenaire et inversement.

Pour les moyennes :
fun5_avg_ect corrélé positivement à attribut5_dist
attribut5_dist corrélé positivement à amb5_avg_act

## Rapport de corrélation

Le rapport de corrélation est un indicateur statistique qui mesure l'intensité de la liaison entre une variable quantitative et une variable qualitative. Plus il est proche de 1, plus elles sont corrélées et plus il est proche de 0, moins elles le sont.

```{r}
#Calcule rapport de correlation pour une variables qualitative par rapport a la variable match
rapp_corr <- function(df,x, groupe) {
    vartot<- sum((df[,x] - mean(df[,x]))^2)
    moyennes <- df%>%group_by(!!rlang::sym(groupe))%>%summarise(n=mean(!!rlang::sym(x)))%>%select(n)
    effectifs <- df%>%group_by(!!rlang::sym(groupe))%>%summarise(l=length(!!rlang::sym(x)))%>%select(l)
    varinter <- (sum(effectifs * (moyennes - mean(df[,x]))^2))
    res <- varinter/vartot
    return(res)
  }
```


```{r}
#Calcul des rapports de correlation
df_num2 = df_num
df_num2$match <- df_mod$match
rap_cor_match <- sapply(colnames(df_num), rapp_corr, df=df_num2, groupe="match")
rap_df <- data.frame("attribut"=names(rap_cor_match), "rap_cor"=rap_cor_match)
rap_df <- rap_df[order(rap_df$rap_cor),]
```


```{r}
ggplot(rap_df, aes(x = reorder(attribut, -rap_cor), y = rap_cor)) + 
  geom_bar(stat = "identity", fill="lightblue") +
  ggtitle("Rapport de corrélation pour chaque variable quantitative, avec le variable 'match' à expliquer") +
  theme(plot.title = element_text(size = 10, face = "bold"), axis.text.x = element_blank()) +
  xlab("Indice des variables quantitatives explicatives")+
  ylab("Rapport de corrélation")
```

Les rapports de corrélation sont très faibles donc les variables sont peu corrélées avec la variable à expliquer : match.

## Test de student

Pour chaque variable, on fait un test de Student : L'hypothèse nulle (H0) dit que les moyennes es observations des variables explicatives ne sont pas significativement différentes selon la classe de la variable explicative.


```{r}
studentTest <- function(x, y){
  return(t.test(x~df_num2$match)$p.value)
}
stud_t_match <- unlist(lapply(df_num, studentTest, y=df_num2$match))

student_t <- data.frame("attribut"=names(stud_t_match), "stud_t"=stud_t_match)
```

```{r}
ggplot(student_t, aes(x = reorder(attribut, -stud_t), y = stud_t)) + 
  geom_bar(stat = "identity", fill="lightgoldenrod") +
  ggtitle("Test de Student pour chaque variable quantitative, avec le variable 'match' à expliquer") +
  theme(plot.title = element_text(size = 10, face = "bold"), axis.text.x = element_blank()) +
  xlab("Indice des variables quantitatives explicatives")+
  ylab("Test de Student")
```
 Toutes les p-value sont < 0.05 donc on rejette H0 pour toutes les variables.
 Les moyennes sont significativement différentes donc on conserve les variables.

## V de Cramer

Le V de Cramer est une mesure d'association entre deux variables de type factor, dans notre cas.
Pour chaque variable on calcule le V de Cramer par rapport à la variable match à expliquer

```{r}
df_fac = df_mod %>% select(where(is.factor))

#Calcul de V de Cramer pour chaque variable avec la variable match à expliquer
cramV <- function(x, y){
  return(cramerV(table(x,y)))
}
cram_v_match <- unlist(lapply(df_fac[,-1], cramV, y=df_fac[,1]))

cram_v <- data.frame("attribut"=names(cram_v_match), "cramer_v"=cram_v_match)
```

```{r}
ggplot(cram_v, aes(x = reorder(attribut, -cramer_v), y = cramer_v)) + 
  geom_bar(stat = "identity", fill="lightgreen") +
  ggtitle("V de Cramer pour chaque variable qualitative, avec le variable 'match' à expliquer") +
  theme(plot.title = element_text(size = 10, face = "bold"), axis.text.x = element_blank()) +
  xlab("Indice des variables qualitatives explicatives")+
  ylab("V de Cramer")
```

Les valeurs sont très peu significatives donc pas de problème !

# Regression logistique

Une autre méthode qu'on peut utiliser est la regression logistique. Il s'agit d'un modèle linéaire généralisé. On utilise le *logit* comme fonction de lien.
Cette méthode souffre du fléau de la dimension et nos données contiennent une centaine de variables. Il est donc plus intéressant d'effectuer une seconde selection de variables sur nos données, pour en retenir quelques dizaines au maximum. Cette fois, on utilise la méthode sbf du package *caret* dédiée à la selection de variables.



On va donc recréer une base d'entrainement et de test.
Dans sbfControl, le paramètre "function" permet d'indiquer quelle méthode est utilisée pour la selection de variables. Nous avons arbitrairement choisi la méthode lda (Linear Discriminant Analysis).


```{r}
filterCtrl <- sbfControl(functions = ldaSBF, method = "repeatedcv", repeats = 5)
rfWithFilter <- sbf(match ~.,data=train_data, sbfControl = filterCtrl) 
```

L'objet rfWithFilter contient de nombreuses données mais on s'intéresse uniquement aux variables selectionnées par *sbf*. On récupère ses variables :
```{r}
var_imp2 = rfWithFilter$variables$selectedVars
var_imp2 = var_imp2[var_imp2 %in% colnames(train_data)]
```

On passe de 110 variables explicatives à 44 variables ! 
On ajuste un modèle de regression logistique sur les nouvelles données obtenues et on observe les performances du modèle.

```{r,echo=FALSE}

#nouvelles données d'entrainement
dapp = train_data[,c("match",var_imp2)]
#nouvelles données de test
dtest = test_data[,c("match",var_imp2)]

#Entrainement regression logistique
RL_model1 = caret::train(
  form = match ~ .,
  data = dapp,
  trControl = trainControl(method = "cv", number = 5,summaryFunction = f1),
  metric = "F1",
  method = "glm")

caret::confusionMatrix(predict(RL_model1,dapp),dapp$match,positive = '1',mode = 'prec_recall')

```

Le modèle est particulièrement mauvais, même sur les données d'apprentissage. On arrive pas à détecter les match positifs.

On utilise donc une méthode d'oversampling ROSE afin d'équilibrer les données (afin de garder une certaine cohérence, on considère 40% de matchs). On augmente la taille de notre échantillon d'apprentissage par la même occasion (on multiplie sa taille par 5).

```{r}
dapp2 <- ROSE(match ~ ., data=dapp, seed=123,N=round(5*nrow(dapp)),p=0.4)$data
```

On ajuste ensuite un modèle de regression logistique (comme précedemment) sur ces nouvelles données. On effectue une opération de scaling sur les données avant.

```{r,echo=FALSE}
finalRL = caret::train(
  form = match ~ .,
  data = dapp2,
  trControl = trainControl(method = "cv", number = 10,summaryFunction = f1),
  metric = "F1",
  method = "glm",
  preProcess = c("scale"))
```

```{r,echo =FALSE}
confusionMatrix(predict(finalRL,dapp),dapp$match,positive = '1',mode = 'prec_recall')
```

Sur les données d'apprentissage, on améliore le F1 Score et la détection de matchs positifs. Cependant cela reste insuffisant. On obtient logiquement des performances similaires sur l'échantillon de validation :

```{r,echo=FALSE}
confusionMatrix(predict(finalRL,dtest),dtest$match,positive = '1',mode = 'prec_recall')
```

La regression logistique n'est clairement pas adaptée à la prédiction pour nos données. Même après une autre selection de variables avec l'AUC, les résultats sont toujours mauvais.

# SVM

L'approche SVM (Support Vector Machines) cherche le meilleur hyperplan en terme de séparation des données. Dans notre cas, avec plus que 100 variables explicatives, il est très probable que les données ne soient pas séparables.

Dans un premier temps nous testons un modèle SVM linéaire.
```{r}
library(e1071)
dtest = test_data
dapp = train_data

tune.out.lin.sim <- tune(svm,match~.,data=dapp,kernel="linear",ranges=list(cost=c(0.1,0.01,0.001)))
summary(tune.out.lin.sim)
mod.svm.lin<-tune.out.lin.sim$best.model
# resultat sur le modèle svm tester
pred.svm.lin = predict(mod.svm.lin, newdata = dtest)
print(caret::confusionMatrix(data=pred.svm.lin,reference=dtest$match,positive="1"))
```
Nous obtenons des résultats pas très corrects, ce modèle a classifié toutes les rencontres comme non-match car la précision est de 0.834. Ce modèle ayant un rappel de 0 et on a trop de Faux Négatifs. 

Pour corriger cet erreur, nous allons réeschantilloner la base avec la méthode Rose permetttant de rééquilibrer les classes. Cette méthode crée une base de donnée équilibrée par des méthodes de bootstrap et d'individus artificielles. Elle donne souvent de bons résulats.
```{r}
dtest = test_data
# base rose
dapp <- ROSE(match ~ ., data = train_data, seed = 1)$data

tune.out.lin.rose <- tune(svm,match~.,data=dapp,kernel="linear",ranges=list(cost=c(0.1,0.01,0.001)))
summary(tune.out.lin.rose)
mod.svm.lin.rose<-tune.out.lin.rose$best.model
# resultat sur le modèle svm tester
pred.svm.lin.rose = predict(mod.svm.lin.rose, newdata = dtest)
print(caret::confusionMatrix(data=pred.svm.lin.rose,reference=dtest$match,positive="1"))

ggplot(data.frame(tune.out.lin.rose$performances[,-3]))+geom_point(aes(log(cost),error))+geom_line(aes(log(cost),error))+labs(title = "Erreur en fonction de log(cout)")

```
Nous obtenons un meilleur modèle. La meilleur valeur pour C est 0.1 et pour ce modèle, 66.3% des rencontres sont correctement prédites et 57% des rencontres match et 68,19% des rencontres non-match sont correctement prédites. Nous calculons le F1 score pour mesurer l'équilibre entre la précision et le rappel. 
```{r}
recall = as.double(caret::confusionMatrix(data=pred.svm.lin.rose,reference=dtest$match,positive="1")$byClass["Sensitivity"])
precision = as.double(caret::confusionMatrix(data=pred.svm.lin.rose,reference=dtest$match,positive="1")$byClass["Pos Pred Value"])
f1score.lin.rose = 2*((precision*recall)/(precision+recall))
f1score.lin.rose
```
Le F1 score du modèle SVM linéaire est de 0.360.

Ensuite on essaie le modèle SVM radial. On cherche à optimiser les parametres de cout et de gamma.
```{r}
tune.out.rad <- tune(svm,match~.,data=dapp,kernel="radial",ranges=list(cost=c(.0001,.001,.01,.1),gamma=c(.01,.1,.5,1,2)))
summary(tune.out.rad)
mod.svm.rad<-tune.out.rad$best.model
# resultat sur le modèle svm tester
pred.svm.rad = predict(mod.svm.rad, newdata = dtest)
print(caret::confusionMatrix(data=pred.svm.rad,reference=dtest$match,positive="1"))

ggplot(data.frame(tune.out.rad$performances[,-3]))+geom_point(aes(log(cost),dispersion))+geom_line(aes(log(cost),dispersion))+labs(title = "Erreur en fonction de log(cout)")

```
Le meilleur valeur pour C est 0.1 et pour gamma est 0.01. Pour ce modèle de SVM radial, 70.6% des rencontres sont correctement prédites et 53% des rencontres match et 74% des rencontres non-match sont correctement prédites. Nous calculons le F1 score pour mesurer l'équilibre entre la précision et le rappel. 
```{r}
recall = as.double(caret::confusionMatrix(data=pred.svm.rad,reference=dtest$match,positive="1")$byClass["Sensitivity"])
precision = as.double(caret::confusionMatrix(data=pred.svm.rad,reference=dtest$match,positive="1")$byClass["Pos Pred Value"])
f1score.rad = 2*((precision*recall)/(precision+recall))
f1score.rad
```
Le F1 score du modèle SVM radial est de 0.374.

```{r}
dtest = test_data
# base rose
dapp <- ROSE(match ~ ., data = train_data, seed = 1)$data

tune.out.pol <- tune(svm,match~.,data=dapp,kernel="polynomial",ranges=list(cost=c(0.1,0.01,0.001),gamma=0.1,degree=c(2,3)))
summary(tune.out.pol)
mod.svm.pol<-tune.out.pol$best.model
# resultat sur le modèle svm tester
pred.svm.pol = predict(mod.svm.pol, newdata = dtest)
print(caret::confusionMatrix(data=pred.svm.pol,reference=dtest$match,positive="1"))

ggplot(data.frame(tune.out.pol$performances[,-3]))+geom_point(aes(log(cost),error))+geom_line(aes(log(cost),error))+labs(title = "Erreur en fonction de log(cout)")

```
Le meilleur valeur pour C est 0.1 et pour le degré est 3. Pour ce modèle de SVM polynomial, 73.7% des rencontres sont correctement prédites et 48% des rencontres match et 78.8% des rencontres non-match sont correctement prédites. Nous calculons le F1 score pour mesurer l'équilibre entre la précision et le rappel. 

```{r}
recall = as.double(caret::confusionMatrix(data=pred.svm.pol,reference=dtest$match,positive="1")$byClass["Sensitivity"])
precision = as.double(caret::confusionMatrix(data=pred.svm.pol,reference=dtest$match,positive="1")$byClass["Pos Pred Value"])
f1score.pol = 2*((precision*recall)/(precision+recall))
f1score.pol
```
Le F1 score du modèle SVM polynomial est de 0.377.

Le meilleur modèle de SVM est le modèle polynomial de degré 3 et de cout égal à 0.1. Pour ce modèle le taux de bien classé est 73.7% avec un F1 score de 0.377.

# Regression de Lasso classique

La Régression de Lasso est une régression pénalisée.
En effet dans l'estimation des paramètres du modèle, on utilise également la méthode des moindres carrés avec une pénalité sur la norme de ces paramètres via un hyper-paramètre LAMBDA, positif.

On entraine donc un modèle sur l'échantillon d'entrainement pour optimiser LAMBDA par Cross Validation.
On retient le LAMBDA qui minimise l'erreur.
Enfin, on réalise une prédiction sur l'échantillon de validation pour obtenir le score du modèle.

```{r}
#IMPORTATION JEU DE DONNEE
df_mod=feature_eng()
#var significatif 
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual) # on peut choisir le seuil pour prendre les meilleurs # variables significatifs

# Creation des bases de test et d'apprentissage

resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest


#nouvelles données d'entrainement
dapp = train_data[,c("match",var_imp2)]
#nouvelles données de test
dtest = test_data[,c("match",var_imp2)]



#Parametres lambda a tester
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

X <- model.matrix(match~., data=dapp)
X_test = model.matrix(match~., data=dtest)
y <- dapp$match

#Entrainement du modele
lasso_cv <- cv.glmnet(X, as.factor(y), alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10, family="binomial")

#Affichage error / lambda
plot(lasso_cv)

#Recuperation du meilleur lambda
lambda_best = lasso_cv$lambda.min

#Model avec meilleur lambda
lasso_mod <- glmnet(X, as.factor(y), alpha = 1, lambda = lambda_best, standardize = TRUE, family="binomial")

#Prediction
pred <- as.numeric(predict(lasso_mod, X_test, type="class"))

#Affichage des resultats avec caret
print(caret::confusionMatrix(data=factor(pred),reference=factor(dtest$match),positive="1",mode = "prec_recall"))
```

L'Accuracy est bonne : 0.83.
Cependant, le rappel (Recall) est très mauvais : cela signifie que le modèle prédit bien les 0 mais mal les 1.
En effet, le jeu de données est déséquilibré car il y a beaucoup plus de 0 que de 1.
Pour pallier à ce problème on fait un travail sur le sampling avec 3 méthodes : Rose, OverSampling & UnderSampling afin d'équilibrer le jeu de données et améliorer les prédictions.


## Sampling sur le jeu de donnees avec ROSE

```{r}
#Sampling avec ROSE pour avoir un jeu de données plus équilibré
data.rose <- ROSE(match ~ ., data = dapp, seed = 1)$data
table(data.rose$match)
```

```{r}
#Modele et test
dapp1 = data.rose

X <- model.matrix(match~., data=dapp1)
X_test = model.matrix(match~., data=dtest)
y <- dapp1$match

#Entrainement du modele
lasso_cv <- cv.glmnet(X, as.factor(y), alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10, family="binomial")

#Affichage error / lambda
plot(lasso_cv)

#Recuperation du meilleur lambda
lambda_best = lasso_cv$lambda.min

#Model avec meilleur lambda
lasso_mod <- glmnet(X, as.factor(y), alpha = 1, lambda = lambda_best, standardize = TRUE, family="binomial")

#Prediction
pred_rose <- as.numeric(predict(lasso_mod, X_test, type="class"))

#Affichage des resultats avec caret
print(caret::confusionMatrix(data=factor(pred_rose),reference=factor(dtest$match),positive="1",mode = "prec_recall"))
```

Avec un sampling Rose, l'Accuracy diminue.
Cependant on passe d'un F1 score de 0.01 à 0.37 : cela est lié au fait que le taux de Vrai Positif est beaucoup plus important. Avec un jeu de données plus équilibré, le modèle est "mieux entrainé" et prédit mieux les 1.

Testons les 2 autres méthodes : Over & Under.

## Over & Under sampling

```{r}
#over sampling dataset
dapp2 <- ovun.sample(match ~ ., data = dapp, method = "over",N = 4696)$data
table(dapp2$match)
#under sampling dataset
dapp3 <- ovun.sample(match ~ ., data = dapp, method = "under", N = 934, seed = 1)$data
table(dapp3$match)
```

### Avec OVER sampling

```{r}
X <- model.matrix(match~., data=dapp2)
X_test = model.matrix(match~., data=dtest)
y <- dapp2$match

#Entrainement du modele
lasso_cv <- cv.glmnet(X, as.factor(y), alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10, family="binomial")

#Affichage error / lambda
plot(lasso_cv)

#Recuperation du meilleur lambda
lambda_best = lasso_cv$lambda.min

#Model avec meilleur lambda
lasso_mod <- glmnet(X, as.factor(y), alpha = 1, lambda = lambda_best, standardize = TRUE, family="binomial")

#Prediction
pred_over <- as.numeric(predict(lasso_mod, X_test, type="class"))

#Affichage des resultats avec caret
print(caret::confusionMatrix(data=factor(pred_over),reference=factor(dtest$match),positive="1",mode = "prec_recall"))
```

Le modèle évalue également mieux les 1, le rappel est donc meilleur que sans travail sur l'équilibrage de jeu de données.
Toutefois, l'accuracy reste faible. 
Pour comparer, on utilise enfin un UNDER sampling.

### Avec UNDER sampling

```{r}
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

X <- model.matrix(match~., data=dapp3)
X_test = model.matrix(match~., data=dtest)
y <- dapp3$match

#Entrainement du modele
lasso_cv <- cv.glmnet(X, as.factor(y), alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10, family="binomial")

#Affichage error / lambda
plot(lasso_cv)

#Recuperation du meilleur lambda
lambda_best = lasso_cv$lambda.min

#Model avec meilleur lambda
lasso_mod <- glmnet(X, as.factor(y), alpha = 1, lambda = lambda_best, standardize = TRUE, family="binomial")

#Prediction
pred_under <- as.numeric(predict(lasso_mod, X_test, type="class"))

#Affichage des resultats avec caret
print(caret::confusionMatrix(data=factor(pred_under),reference=factor(dtest$match),positive="1",mode = "prec_recall"))
```

Le modèle prédit mieux les 1 que sans travail sur le sampling mais l'accuracy reste faible.
Désormais, étudions d'autres modèles susceptibles d'obtenir de meilleurs scores.

### Pour chaque methode de sampling : rose, over & under, on affiche la courbe ROC.

```{r}
roc1=roc(dtest$match, pred_rose)
roc2=roc(dtest$match, pred_over)
roc3=roc(dtest$match, pred_under)

plot(roc1, col = 1, lty = 2, main = "ROC")
plot(roc2, col = 2, lty = 2, add = TRUE)
plot(roc3, col = 3, lty = 2, add = TRUE)

legend("bottom",
       legend=c("Rose", "Over", "Under"),
       col=c(1,2,3),
       lwd=4, cex =1, xpd = TRUE, horiz = FALSE)
```

Meme avec un sampling les résultats obtenus ne sont pas satisfaisants : les modèles prédisent trop de 0 et le "rappel" n'est pas bon. On s'oriente donc vers d'autres modèles.

# XGBOOST
Dans cette partie on va s'intérésser à une méthode de modélisation utilisant l'agrégation par boosting. Pour cela on utiliser comme modèle de base l'arbre de décision. Les hyperparamètres à optimiser sont :

** le nombre d'itération du boosting
** la profondeur maximale des arbres 
** le pas de descente de gradient
** le pourcentage de variable qu'on garde pour la construction de chaque arbre
** la taille des sous-échantillons
** le gain minimum pour diviser un noeud

Afin d'optimiser ces hyperparamètres, nous utiliserons comme métric le F1-score. De plus, nous comparerons les performance de notre modèle avec et sans méthode de ré-échantillonage

```{r}
f1 <- function (data, lev = NULL, model = NULL) {
  setTxtProgressBar(pb, k/nb_tune);k<<-k+1;
  precision <- Precision(data$pred, data$obs,positive ="1")
  recall  <- Recall(data$pred, data$obs,positive ="1")
  f1_val <- F1_Score(data$pred, data$obs,positive ="1")
  resu=c("precision"=precision,"rappel"=recall,"F1"=f1_val)
  resu
}

test_model<-function(test_data,model,seuil=0.5){
  # Testing
  final <- data.frame(actual = test_data$match,predict(model, newdata = test_data, type = "prob"))
  final$predict <- ifelse(final$X0 > seuil, "0", "1")
  caret::confusionMatrix(as.factor(final$predict), test_data$match,positive="1",mode="prec_recall")
}

best_f1<-function(model,test_data,min=0.01,max=0.99,step=0.001){
  seuil=seq(min,max,step)
  f1_seuil=sapply(seuil,function(x){
    #print(x)
    # Testing
    final <- data.frame(actual = test_data$match,predict(model, newdata = test_data, type = "prob"))
    final$predict <- factor(ifelse(final$X0 > x, "0", "1"),levels = c("0","1"))
    c("seuil"=x,
      "precision"=Precision(final$predict, final$actual,positive ="1"),
      "rappel"=Recall(final$predict, final$actual,positive ="1"),
      "F1"=F1_Score(final$predict, final$actual,positive ="1"))
  })
  return(f1_seuil)
}

```

```{r}
tune_grid <- expand.grid(nrounds=c(500,1000,2000),max_depth = c(1:3), eta = c(0.1,0.01), gamma = c(0.1,0),colsample_bytree = c(0.75),subsample = c(0.50),min_child_weight = c(0))
nb_tune<<-nrow(tune_grid)*10

#RE IMPORTATION JEU DE DONNEE
df_mod=feature_eng()
#var significatif 
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual)
#Split
resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest

# Train controle----------------------------------------------
ctrl <- trainControl(method = "cv",number = 10,summaryFunction = f1,search = "grid")
ctrl_up <- trainControl(method = "cv",number = 10,summaryFunction = f1,search = "grid",sampling = "up")
ctrl_down <- trainControl(method = "cv",number = 10,summaryFunction = f1,search = "grid",sampling = "down")

#XGBOOST
k<<-0;pb<<-txtProgressBar(style = 3,width = 50)
rf_fit <- caret::train(match ~., data = train_data, method = "xgbTree",trControl=ctrl,tuneGrid = tune_grid,tuneLength = 10,preProcess = c("scale", "center"),metric = "F1")
plot(rf_fit)
# Testing
f1_eval=as.data.frame(t(best_f1(rf_fit,test_data)))
test_model(test_data,rf_fit,f1_eval$seuil[which.max(f1_eval$F1)])

#XGBOOST Sur -echantillonnage
k<<-0;pb<<-txtProgressBar(style = 3,width = 50)
rf_fit_up <- caret::train(match ~., data = train_data, method = "xgbTree",trControl=ctrl_up,tuneGrid = tune_grid,tuneLength = 10,preProcess = c("scale", "center"),metric = "F1")
plot(rf_fit_up)
# Testing
f1_eval2=as.data.frame(t(best_f1(rf_fit_up,test_data)))
#plot(f1_eval2$seuil,f1_eval2$F1)
test_model(test_data,rf_fit_up,f1_eval2$seuil[which.max(f1_eval2$F1)])

#XGBOOST Echantillonnage down
k<<-0;pb<<-txtProgressBar(style = 3,width = 50)
rf_fit_down <- caret::train(match ~., data = train_data, method = "xgbTree",trControl=ctrl_down,tuneGrid = tune_grid,tuneLength = 10,preProcess = c("scale", "center"),metric = "F1")
plot(rf_fit_down)
# Testing
f1_eval3=as.data.frame(t(best_f1(rf_fit_down,test_data)))
#plot(f1_eval3$seuil,f1_eval3$F1)
test_model(test_data,rf_fit_down,f1_eval3$seuil[which.max(f1_eval3$F1)])

#Comaraison
f1_eval$methode="normal"
f1_eval2$methode="sur-échantillonnage"
f1_eval3$methode="sous-échantillonnage"

comparaison = bind_rows(f1_eval,f1_eval2,f1_eval3)
ggplot(comparaison, aes(x=seuil,y=F1,color=methode))+geom_line()+ggtitle("Comparaison des algorithme xgboost")

ggplot(comparaison, aes(x=precision,y=rappel,color=methode))+geom_line()+ggtitle("Courbe précision/rappel")




```


# Réseaux de neurones

Les réseaux de neurones est une outil très puissant pour la classification binaire. Pour réexpliquer ce qu'est un réseau de neurones, ce sont plusieurs couches de neurones connectés (ici totalement connectés) avec un couche d'entrées et une couche de sortie. Les informations vont passer dans le réseau de neurones afin d'y être transformé par de multiples combinaison et à l'aide de fonction d'activation. Par backpropagation du gradient de la fonction de perte, à chaque batch, le réseau de neurones est censé améliorer ces différents poids pour avoir de meilleurs résultats. Nous allons voir si les réseaux de neurones fonctionnent bien en faisant 4 types de modèles différents. Le premier en changeant la fonction de perte pour donner plus d'importance à la classe 1, le deuxième en mettant des poids plus important à la classe en prenant la fonction de perte standard (binary cross_entropy) et les deux dernières avec des méthodes de rééchantillonage : Rose et suréchantillonage.

## Transformation de la base de donnée pour l'intelligence artificielle

```{r, include=FALSE}
# vous pouvez choisir le nombre d'epochs que vous voulez (plus d'epochs plus lent)
# ne pas dépassez 2000 environ pour éviter le surapprentissage

epochs = 500
```


```{r, include=FALSE}
#IMPORTATION JEU DE DONNEE
df_mod=feature_eng()
#var significatif 
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual) # on peut choisir le seuil pour prendre les meilleurs # variables significatifs

# Creation des bases de test et d'apprentissage

resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest
```

Les bases de données doivent transformé en numérique puis en matrix. 
```{r }
dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))


ncol = ncol(dapp) - 1 
xtrain = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain = dapp$match


xtest = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
ytest = dtest$match

#Definition des metric pour l'IA

recall_m=function(y_TRUE, y_pred){
  TRUE_positives = k_sum(k_round(k_clip(y_TRUE * y_pred, 0, 1)))
  possible_positives = k_sum(k_round(k_clip(y_TRUE, 0, 1)))
  recall = TRUE_positives / (possible_positives + k_epsilon())
  return(recall)
} 
precision_m=function(y_TRUE, y_pred){
  TRUE_positives = k_sum(k_round(k_clip(y_TRUE * y_pred, 0, 1)))
  predicted_positives = k_sum(k_round(k_clip(y_pred, 0, 1)))
  precision = TRUE_positives / (predicted_positives + k_epsilon())
  return(precision)
}

# parametre beta a changer
f1_m=function(y_TRUE, y_pred){
  beta = 1
  precision = precision_m(y_TRUE, y_pred)
  recall = recall_m(y_TRUE, y_pred)
  return((1+beta^2)*((precision*recall)/(precision+beta * recall+k_epsilon())))
} 
```

## Intelligence artificielle avec la fonction focal_loss

La fonction de perte focal_loss est connue pour s'occuper de données non équilibrées. En prenant une valeur de gamma grande, on fait en sorte que les bonnes prédiction pour la classe 0 soit moins importantes cependant en augmantant gamma, on diminue fortement la fonction de perte ansi, au commencement même de l'entrainement, on peut avoir une fonction de perte proche de 0 et ainsi, pas efficace. Il faut donc trouver le meilleur gamma capable de donner plsu importance aux bonnes prédictions pour la classe 1 sans avoir une fonciton de perte trop petite et inutilisable.

${p}_{t} = y_{pred} * y_{obs} + (1-y_{pred}) * (1-y_{obs}) $
$FL = (1-{p}_{t})^{\gamma}-log({p}_{t})$

$\gamma$ est un paramètre à optimiser

```{r, results='hide'}

#Optimisation de gamma

# resultat pour gamma
# utiliser cette commande pour optimiser le choix de gamma / cette méthode a ces limites car ne fait une fois l'expérience sur une valeur de gamma et n'est pas en validation croisée. Cela serait trop long et compliqué sinon
#res = optimisationNeuroneDeeplearning(0.2, 10, 0.5, 700, xtrain, xtest,ytrain, ytest);


# loss function et metrics avec choix de gamma

choix_gamma = 2

focal_loss=function(y_TRUE, y_pred){
  gamma = choix_gamma
  
  print("gamma")
  pt = y_pred * y_TRUE + (1-y_pred) * (1-y_TRUE)
  print("pt")
  pt = k_clip(pt, 0, 1)
  print("pt2")
  CE = -k_log(pt+k_epsilon())
  print("CE")
  FL = k_pow(1-pt, gamma) * CE
  print("FL")
  loss = k_sum(FL, axis=1)
  print(loss)
  return(loss)
}


# creation du modèle / on prend ici un modèle simple mais tout de meme assez compliqué pour résoudre cette tache

model <- keras_model_sequential()
model %>% 
  layer_dense(units = round(ncol), input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.7),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.5),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.3), activation ="relu") %>%
  layer_dropout(0.3)  %>%
  
  layer_dense(units = 1, activation = "sigmoid")




# compilation, apprentissage et prédiction

model %>% compile(
  loss = focal_loss, # utilisation de cette nouvelle foncton de perte
  optimizer = 'adam',
  metrics = f1_m # metrique utilisé : f1 score
)

history <- model %>% fit(
  xtrain,  ytrain, 
  
  batch_size =0.05,epochs = 2*epochs,# a besoin de plus d'epochs que les autres
  validation_split = 0.2,
  view_metrics = FALSE
)

```

## Résultats avec la fonction focal_loss


```{r,echo=TRUE}

plot(history)

#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))

```


Les résultats sont assez mauvais, le problème étant que le modèle est très instable à cause de la fonction de perte et même en changeant le gamma, on obtient encore des résultats médiocres. Passons donc à une autre facon de modéliser ce problème.

## Utilisation de poids

On va ici mettre des poids aux classes tout en utilisant la fonction de perte usuelles pour les classifieurs binaires soit la "binary crossentropy" (fonction de perte de l'exemple du cours). La valeur du poids à mettre à la classe i est juste le nombre d'indivus de la classs majoritaire divisé par le nombre d'individus de classe i. On obtient ici pour la classe 0 : 1 et pour la classe 1 : 6.0284857571. Mettons en place ce nouveau modèle :

```{r, results='hide'}

# poids


model <- keras_model_sequential()
model %>% 
  layer_dense(units = round(ncol*0.8), input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15), activation ="relu") %>%
  layer_dropout(0.3)  %>%
  
  layer_dense(units = 1, activation = "sigmoid")



model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
  
)

history <- model %>% fit(
  xtrain,  ytrain, 
  
  batch_size =0.05,epochs = epochs,
  validation_split = 0.2,
  class_weight=list("0"=1,"1"=6.0284857571),
  view_metrics = FALSE
)
```

## Résultats avec les poids

```{r,echo=TRUE}

plot(history)


#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))
```
On obtient de bien meilleurs résulats, on peut changer le nombre d'epoch pour essayer d'obtenir encore de meilleur résultats. epochs à 200 n'est pas le meilleur paramètre, il est juste à cette valeur pour éviter d'avoir trop à attendre.Voici le meilleur modèle de poids :


```{r,echo=TRUE}

model_poids = load_model_hdf5("Modelisation/IA/model_f1_poids_35806_2.hdf5")

l = seq(0.1,0.9,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_poids,xtest)>s), ytest, positive = "1"))

meilleur_seuil_poids = l[which.max(f1_list)]
f1_poids = f1_list[which.max(f1_list)]

print("prédiction avec le seuil standard de 0.5")
predSimple = as.integer(predict_proba(model_poids,xtest)>meilleur_seuil_poids)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))
print("prédiction avec le seuil standard de 0.5")
predSimple = as.integer(predict_proba(model_poids,xtest)>0.5)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))

```
## Intelligence artificielle avec méthode de Sampling : Rose

Passons maintenant aux différentes méthode d'échantillonange permetttant de ré équilibrer les classes et commencant par la méthode d'échantillonage ROSE. Cette méthode crée une base de donnée équilibré. Elle donne souvent de bons résulats.

## Création des différentes bases nécessaires

On change de base car il faut moins de variables pour le nombre d'individus pour que les méthodes de rééchantillonage fonctionne.

```{r}
df_mod=feature_eng()
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual,0.1) # on change ici le seuil car Rose a besoin #d'avoir assez d'individus pour le nombre de variables donc on diminue le nombre de varaibles 
#Split
resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest
#base normale
dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))
ncol = ncol(dapp) - 1 
xtrain_normal = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_normal = dapp$match
xtest_normal = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
ytest_normal = dtest$match
# base rose
dapp <- ROSE(match ~ ., data = train_data, seed = 1)$data
dapp = data.frame(lapply(dapp, function(x) as.numeric(as.character(x))))
ncol = ncol(dapp) - 1 
xtrain_rose = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_rose = dapp$match
```

## Resultat avec Rose
```{r,  results='hide'}
# creation du modèle
model <- keras_model_sequential()
model %>% 
  layer_dense(units = ncol, input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.7), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.3),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = 1, activation = "sigmoid")
# compilation, apprentissage et prédiction
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',#optimizer_rmsprop()
  metrics = 'accuracy'
)
history <- model %>% fit(
  xtrain_rose,  ytrain_rose, 
  batch_size =0.1,epochs = epochs,
  validation_split = 0.1,
  view_metrics = FALSE
)
```

```{r,echo=TRUE}
plot(history)
#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest_normal)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
```

On obtient de très bons modèles avec cette méthode ci et c'est plutôt facile à implémenter.

##Meilleur modèle avec Rose trouvée :

```{r,echo=TRUE}
# méthode Rose
model_rose = load_model_hdf5("Modelisation/IA/m_38_rose.hdf5")
l = seq(0.3,0.7,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_rose,xtest_normal)>s), ytest_normal, positive = "1"))
meilleur_seuil_rose = l[which.max(f1_list)]
f1_rose = f1_list[which.max(f1_list)]
predSimple = as.integer(predict_proba(model_rose,xtest_normal)>meilleur_seuil_rose)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
```


## Intelligence artificielle avec méthode d'échantillonage : suréchantillonage

Pour les mêmes raisons, on change de base et on utilise de l'oversampling "simplement".

Création des différentes bases nécessaires
```{r}
df_mod=feature_eng()
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual,0.1)
#Split
resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest
# base oversampling
dapp <- ovun.sample(match ~ ., data = train_data, method = "over",N = 2*(nrow(train_data) - sum(as.numeric(train_data$match)-1)))$data
table(dapp$match)
dapp = data.frame(lapply(dapp, function(x) as.numeric(as.character(x))))
ncol = ncol(dapp) - 1 
xtrain_over = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_over = dapp$match
```

## Résultat avec oversampling

```{r,  results='hide'}
model <- keras_model_sequential()
model %>% 
  layer_dense(units = round(ncol*0.5), input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.5), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = 1, activation = "sigmoid")
# compilation, apprentissage et prédiction
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)
history <- model %>% fit(
  xtrain_over,  ytrain_over, 
  batch_size =0.1,epochs = epochs,
  validation_split = 0.1
)
```

```{r,echo=TRUE}
plot(history)
#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest_normal)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```

Les résultats sont très bons avec cette méthode. Mais un peu moins performant que Rose.

## Meilleurs modèles avec la méthode de suréchantillonage :


Modèle à 500 epochs :
```{r}
model_over_1 = load_model_hdf5("Modelisation/IA/m_380_over.hdf5")
l = seq(0.05,0.9,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_over_1,xtest_normal)>s), ytest_normal, positive = "1"))
meilleur_seuil_over_1 = l[which.max(f1_list)]
f1_over_1 = f1_list[which.max(f1_list)]
```
```{r,echo=TRUE}
print("prediction avec le meilleur seuil de f1_score")
predSimple = as.integer(predict_proba(model_over_1,xtest_normal)>meilleur_seuil_over_1)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print("prédiction avec le seuil standard de 0.5")
predSimple = as.integer(predict_proba(model_over_1,xtest_normal)>0.5)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
```
## Comparaison des modèles


Meilleur modèle : XGBoost avec oversampling suivi de IA avec Rose puis SVM, Lasso, logistique.

## Combinaison de modèles IA
Nous avons vu pleins de méthodes pour contrer le déséquilibre des classes et les méthodes rééchantillonages paraissent vraiment adéquates. Nous allons maintenant essayer de combiner les meilleurs modèles soient les modèles avec IA et XGBoost afin que les avantages des uns rattrapent les inconvénients des autres. Pour cela, nous allons effectué un vote à la majorité en pondérant ces résultats par rapport aux meilleurs F1_score. Les meilleurs seuils seront aussi utilisé à chaque fois pour maximiser le F1_score.


## Vote à la majorité et meilleur seuil
On fait ce même vote à la majorité avec les 3 mêmes IA mais les différents meilleurs seuils qui maximisent le F1_score que nous avons trouvé précédemment.On trouve ici un F1_score de 0.5928 ce qui est le meilleur modèle pour l'instant trouvé.
```{r}
score = (f1_rose * 
           as.integer(predict_proba(model_rose,xtest_normal)>meilleur_seuil_rose)
        +f1_over_1 * 
          as.integer(predict_proba(model_over_1,xtest_normal)>meilleur_seuil_over_1)
        +f1_poids * 
          as.integer(predict_proba(model_poids,xtest)>meilleur_seuil_poids)
        + f1_eval$F1[which.max(f1_eval$F1)] *
          as.integer(data.frame(actual = test_data$match,predict(rf_fit, newdata = test_data, type = "prob"))$X0 < f1_eval$seuil[which.max(f1_eval$F1)])
        + f1_eval2$F1[which.max(f1_eval2$F1)] * 
          as.integer(data.frame(actual = test_data$match,predict(rf_fit_up, newdata = test_data, type = "prob"))$X0 < f1_eval2$seuil[which.max(f1_eval2$F1)])
        + f1_eval3$F1[which.max(f1_eval3$F1)] * 
          as.integer(data.frame(actual = test_data$match,predict(rf_fit_down, newdata = test_data, type = "prob"))$X0 < f1_eval3$seuil[which.max(f1_eval3$F1)]))/(f1_rose
    +f1_over_1
    +f1_poids
    +f1_eval$F1[which.max(f1_eval$F1)]
    +f1_eval2$F1[which.max(f1_eval2$F1)]
    +f1_eval3$F1[which.max(f1_eval3$F1)])



l = seq(0.05,0.9,0.001)
f1_list = sapply(l, function(s) F1_Score(as.integer(score>s), ytest, positive = "1"))
meilleur_seuil_combi = l[which.max(f1_list)]
f1_combi = f1_list[which.max(f1_list)]

predSimple = as.integer(score > meilleur_seuil_combi)
```
```{r,echo=TRUE}
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1"))
```


On obtient un super modèle avec un très bon F1 score de 0.47 et une bonne exactitude. Il est le meilleur modèle à ce jour. Ceci n'est qu'une ébauche qu'il serait intéressant d'approfondir.

