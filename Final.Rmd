---
title: "Dating_data : la data qui vous date"
author: "Le super groupe de travaille fada de MACHINE LEARNINNNNNNG"
date: "26/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart)
library(xgboost)
library(ROSE)
library(MLmetrics)
library(caret)
library(doSNOW)
library(dplyr)
library(stringr)
library(rcompanion)

library(tensorflow)
#install_tensorflow() # si jamais, utiliser tensorflow, il faut l'installer avec cette commande

library(keras)
```


# Fonctions nécessaire pour le bon déroulement du projet

```{r}

# Gestion Variables

# Undergra
gestionUndergra = function(df){
  df$undergra = as.character(df$undergra)
  df$undergra[which(df$undergra!="")] = 1
  df$undergra[which(df$undergra=="")] = 0
  df$undergra = factor(df$undergra)
  
  return(df)
}


# Career
gestionCareer = function(df){
  #Si career : lawyer alors career_c : 1
  df = df %>% mutate(career_c = ifelse(career=="lawyer" | career=="law", 1, career_c))
  
  #Si career : Economist alors career_c : 7 (with Finance...)
  df = df %>% mutate(career_c = ifelse(career=="Economist", 7, career_c))
  
  #Si career : tech professional  alors career_c : 7 (with Finance...)
  df = df %>% mutate(career_c = ifelse(career=="tech professional", 5, career_c))
  
  #Suppression des autres individus : quasiment toutes leurs variables sont nulles
  to_delete = df$career_c %>% is.na %>% which
  df = df[!seq_len(nrow(df)) %in% to_delete, ]
  
  df %>% dplyr::select(c("career_c","career"))
  
  return(df)
}



# Income
gestionIncome = function(df){
  income_ = as.character(factor(df$income)) # on convertit en character les valeurs
  
  for(i in 1:dim(df)[1]){
    string = income_[i] # on prend notre character
    id  = nchar(string) - 6 # l'indice où est la virgule
    if(id>0 & !is.na(id)){
      str_sub(string, id, id) <- "" # on remplace la virgule par rien
    }
    income_[i] = string
  }
  
  df$income = as.numeric(income_) # on change toutes les valeurs comme il faut dans le dataframe
  
  return(df)
}



# Essayer d'enlever les Na

# Agregation
agregationVariable = function(df){
  df2=df
  # CODE POUR GERER LE REGROUPEMENT DE VARIABLE
  attribut=c("attr","sinc","intel","fun","amb","shar")
  time=c(as.character(1:3),"s")
  catg=c(1:5,7)
  
  var_gen=outer(attribut,outer(catg,paste0("_",time),paste0),paste0)
  var_avg=outer(attribut,outer(catg,"_avg",paste0),paste0)
  
  #Etape 1 : harmoniser le systeme de point
  for(k in c(1,2,4)){#theme qui ont besoin d'être harmonisé
    for(i in 1:4){
      if(all(var_gen[,k,i]%in%names(df2))){
        df2[,var_gen[,k,i]]=sapply(var_gen[,k,i],function(v) 100*df2[,v]/apply(df2[,var_gen[,k,i]],1,sum,na.rm = T)) 
      }
    }
  }
  
  #ETAPE2 : regroupement
  for(k in 1:6){#catg/
    for(i in 1:6){#attribut
      v=var_gen[i,k,][which(var_gen[i,k,]%in%names(df2))]
      df2[,var_avg[i,k,1]]= apply(df2[,v],1,mean,na.rm = T)
    }
  }
  
  #ETAPE3 : Supression des anciennes variables
  v=as.vector(var_gen)
  v=v[which(v%in%names(df2))]
  df2=df2[,-which(names(df2)%in%v)]
  
  remove(attribut, catg, i, k, time, v, var_avg, var_gen)
  
  return(df2)
}


aggregateCarrer=function(df_individu){
  #carrer_cd
  law=c(1)
  science=c(4,5,12)
  huma=c(3,9,11,13,16)
  art=c(6,17)
  business=c(7,8)
  chercheur=c(2)
  sportif=c(14)
  autre=c(10,15)
  
  df_individu$career_c2=8
  df_individu$career_c2[df_individu$career_c%in%law]=1 #inutile mais bon
  df_individu$career_c2[df_individu$career_c%in%science]=2
  df_individu$career_c2[df_individu$career_c%in%huma]=3
  df_individu$career_c2[df_individu$career_c%in%art]=4
  df_individu$career_c2[df_individu$career_c%in%business]=5
  df_individu$career_c2[df_individu$career_c%in%chercheur]=6
  df_individu$career_c2[df_individu$career_c%in%sportif]=7
  
  return(df_individu)
}

aggregateField=function(df_individu){
  #field_cd
  law=c(1)
  science=c(2,4,5,10)
  huma=c(3,6,7,16,13,9)
  art=c(14,15,17)
  business=c(8)
  autre=c(12,17)
  
  df_individu$field_cd2=6
  df_individu$field_cd2[df_individu$field_cd%in%law]=1 #inutile mais bon
  df_individu$field_cd2[df_individu$field_cd%in%science]=2
  df_individu$field_cd2[df_individu$field_cd%in%huma]=3
  df_individu$field_cd2[df_individu$field_cd%in%art]=4
  df_individu$field_cd2[df_individu$field_cd%in%business]=5
  
  return(df_individu)
}

aggregateActivitie=function(df_individu){
  #activite1
  df_individu$act_sports=apply(df_individu,1,function(x) mean(x[c("sports","tvsports","exercise","hiking","yoga","clubbing")],na.rm=T))
  df_individu$act_art=apply(df_individu,1,function(x) mean(x[c("museums","art","reading","theater","movies","concerts","music")],na.rm=T))
  df_individu$act_autre=apply(df_individu,1,function(x) mean(x[c("dining","gaming","tv","shopping")],na.rm=T))
  
  #activite2
  df_individu$act_casanier=apply(df_individu,1,function(x) mean(x[c("tvsports","gaming","tv","yoga","art","reading","music","movies")],na.rm=T))
  df_individu$act_sortie=apply(df_individu,1,function(x) mean(x[c("sports","museums","theater","concerts","exercise","hiking","clubbing","dining","shopping")],na.rm=T))
  
  return(df_individu)
}

# Remplissage par la moyenne
gestionDesNASystematiques = function(df_individu){
  
  for(var in c("age","field_cd", "date", "amb2_avg", "shar2_avg")){
    id = which(is.na(df_individu[,var]))
    if(length(id)>0){
      df_individu = df_individu[-id,]
    }
  }
  
  for(var in c("attr4_avg", "sinc4_avg", "intel4_avg", "fun4_avg", "amb4_avg", "shar4_avg", "attr5_avg", "sinc5_avg", "intel5_avg", "fun5_avg", "amb5_avg")){
    id = which(is.na(df_individu[,var]))
    m = mean(df_individu[-id,var])
    df_individu[id,var] = m
  }
  
  return(df_individu)
}






creationDf_Couple = function(df,df_individu){
  df_couple_id = df[,c('iid','pid', 'match',"gender")]
  df_couple_id=df_couple_id[order(df_couple_id$gender),]
  
  coupleDated = c()
  ligneAenlever = c()
  
  for(i in 1:dim(df_couple_id)[1]){
    iid = df_couple_id$iid[i]
    pid = df_couple_id$pid[i]
    
    strCouple = paste(as.character(iid),as.character(pid))
    strCoupleInv = paste(as.character(pid),as.character(iid))
    
    if(!(strCoupleInv %in% coupleDated)){
      coupleDated = c(coupleDated,strCouple)
    }
    else{
      ligneAenlever = c(ligneAenlever,i)
    }
  }
  df_couple_id= df_couple_id[-ligneAenlever,]%>%select(-c("gender"))
  
  df_couple = merge(merge(df_couple_id, df_individu, by = 'iid'), df_individu, by.x = 'pid', by.y = 'iid')
  
  
  return(df_couple)
}

######################################################################

creation_avg_ect=function(df_couple2){
  var_num=c("act_sortie","act_casanier","act_autre","act_art","act_sports","age","imprace","imprelig",
            "sports", "tvsports","exercise","dining","museums","art","hiking","gaming","clubbing","reading","tv","theater","movies","concerts","music","shopping","yoga",
            paste0("attr",1:5,"_avg"),paste0("sinc",1:5,"_avg"),paste0("intel",1:5,"_avg"),paste0("fun",1:5,"_avg"),paste0("amb",1:5,"_avg"),paste0("shar",c(1,2,4),"_avg")
  )
  
  #Calcule moy et ect
  for(v in var_num){
    df_couple2[,paste0(v,"_moy")]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,mean,na.rm=F)
    df_couple2[,paste0(v,"_ect")]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,function(x) abs(x[1]-x[2]))
  }
  
  # Calcule de distance
  #activite
  activite=c("sports", "tvsports","exercise","dining","museums","art","hiking","gaming","clubbing","reading","tv","theater","movies","concerts","music","shopping","yoga")
  df_couple2$act_dist=apply(df_couple2[,paste0(activite,"_ect")]^2,1,function(x) sqrt(sum(x)))
  #activite1
  df_couple2$act1_dist=apply(df_couple2[,paste0(c("act_sortie","act_casanier"),"_ect")]^2,1,function(x) sqrt(sum(x)))
  #activite2
  df_couple2$act2_dist=apply(df_couple2[,paste0(c("act_autre","act_art","act_sports"),"_ect")]^2,1,function(x) sqrt(sum(x)))
  
  #Attribut
  attribut=c("attr","sinc","intel","fun","amb","shar")
  #Ce que tu cherches chez l'autre
  df_couple2$attribut1_dist=apply(df_couple2[,paste0(attribut,"1_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses que les gens de sexe opposé cherchent chez l'autre
  df_couple2$attribut2_dist=apply(df_couple2[,paste0(attribut,"2_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses de toi
  df_couple2$attribut3_dist=apply(df_couple2[,paste0(attribut[-6],"3_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que tu penses que ton sexe cherche chez l'autre
  df_couple2$attribut4_dist=apply(df_couple2[,paste0(attribut,"4_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  #Ce que les autres pensent de toi
  df_couple2$attribut5_dist=apply(df_couple2[,paste0(attribut[-6],"5_avg_ect")]^2,1,function(x) sqrt(sum(x)))
  
  df_couple2$correspondance1_dist=apply((df_couple2[,paste0(attribut[-6],"1_avg.x")]-df_couple2[,paste0(attribut[-6],"3_avg.y")])^2,
                                        1,function(x) sqrt(sum(x)))
  
  df_couple2$correspondance2_dist=apply((df_couple2[,paste0(attribut[-6],"1_avg.y")]-df_couple2[,paste0(attribut[-6],"3_avg.x")])^2,
                                        1,function(x) sqrt(sum(x)))
  
  
  var_num=unlist(lapply(var_num,function(x) paste0(x,c(".x",".y","_moy","_ect"))))
  var_num=c(var_num,"act_dist","act1_dist","act2_dist","attribut1_dist","attribut2_dist","attribut3_dist","attribut4_dist","attribut5_dist","correspondance1_dist","correspondance2_dist")
  
  
  return(list("df"=df_couple2, "var_num" =  var_num))
}

creation_same_ind=function(df_couple2){
  
  var_qual=c("field_cd","race", "goal","date","go_out","career_c")
  var_qual2=unlist(lapply(var_qual,function(x) paste0(x,c(".x",".y"))))
  
  
  #Variable qualitative Solution1: varaible somme de modalite + indicatrice same modalitee
  for( v in var_qual){
    v_modalite=unique(c(df_couple2[,paste0(v,".x")],df_couple2[,paste0(v,".y")]))
    #indicatrice same
    var_qual2=c(var_qual2,paste0("same_",v))
    df_couple2[,paste0("same_",v)]= as.numeric(df_couple2[,paste0(v,".x")]==df_couple2[,paste0(v,".y")])
    #comptage
    for(mod in v_modalite){
      var_qual2=c(var_qual2,paste0(v,"_",mod))
      df_couple2[,paste0(v,"_",mod)]=apply(df_couple2[,paste0(v,c(".x",".y"))],1,function(x) sum(c(x[1]==mod,x[2]==mod),na.rm = T))
    }
  }
  for(var in c("match",var_qual2)){
    df_couple2[,var] = as.factor(df_couple2[,var])
  }
  
  return(list("df"=df_couple2, "var_qual" =  var_qual2))
}


feature_selection=function(df_couple2,var_num,var_qual2,seuil=0.05,nb_var_max=Inf){
  if(length(seuil)==1){
    seuil=c(seuil,seuil)
  }
  if(length(nb_var_max)==1){
    nb_var_max=c(nb_var_max,nb_var_max)
  }
  #Numerique
  var_num_sign=sapply(var_num,function(v)  t.test(df_couple2[,v]~df_couple2$match)$p.value)
  var_num_sign=var_num_sign[order(var_num_sign)]
  var_num_sign=var_num_sign[var_num_sign<=seuil[1]]
  var_num_sign=var_num_sign[1:min(nb_var_max[1],length(var_num_sign))]
  barplot(var_num_sign,main = "Test student variable numerique")
  
  
  #Qualitative
  #var_qual_sign=sapply(var_qual2,function(v) {cramerV(table(df_couple2[,c("match",v)]))})
  
  var_qual_sign=sapply(var_qual2,function(v)  chisq.test(df_couple2[,v],df_couple2$match)$p.value)
  var_qual_sign=var_qual_sign[order(var_qual_sign)]
  var_qual_sign=var_qual_sign[var_qual_sign<=seuil[2]]
  var_qual_sign=var_qual_sign[1:min(nb_var_max[2],length(var_qual_sign))]
  
  return(c("match",names(c(var_qual_sign,var_num_sign))))
}

# creation de df_mod, dapp et dtest

# noFactor sert pour le réseau de neurone, un réseaud de neurone ne peut pas prendre de factor
creationDesData = function(df_couple2, varSignificatifs = FALSE, varSupp = FALSE, noFactor = FALSE,p_split=0.7){
  if(length(varSignificatifs) > 1){
    df_mod = df_couple2[,which(names(df_couple2) %in% varSignificatifs)]
  }
  
  if(length(varSupp) > 1){
    if(length(which(names(df_mod) %in% varSupp))>0){
      df_mod =df_mod[,-which(names(df_mod) %in% varSupp)]
    }
  }
  
  if(noFactor){
    for(var in varFactor){
      if(var %in% names(df_mod)){
        df_mod[,var] = as.numeric(df_mod[,var]) -1
      }
    }
    remove(var)
    
    df_mod[,which(names(df_mod) %in% varFactor)] = df_mod[,which(names(df_mod) %in% varFactor)] -1
    df_mod$match = df_mod$match + 1
  }
  
  
  set.seed(1234)
  perm <- createDataPartition(df_mod$match, p = p_split, list = FALSE)
  dapp <- df_mod[perm,]
  dtest <- df_mod[-perm,]
  
  
  return(list(df_mod = df_mod,dapp = dapp,dtest = dtest))
  
}



optimisationNeuroneDeeplearning = function(gamma_min, gamma_max, gamma_pas, epoch, xtrain, xtest,ytrain, ytest){
  sol = as.data.frame(c(0,0,0))
  
  for( i in seq(gamma_min, gamma_max, gamma_pas)){
    
    focal_loss=function(y_true, y_pred){
      gamma = i 
      print("gamma")
      pt = y_pred * y_true + (1-y_pred) * (1-y_true)
      print("pt")
      pt = k_clip(pt, 0, 1)
      print("pt2")
      CE = -k_log(pt+k_epsilon())
      print("CE")
      FL = k_pow(1-pt, gamma) * CE
      print("FL")
      loss = k_sum(FL, axis=1)
      print(loss)
      return(loss)
    }
    
    model <- keras_model_sequential()
    model %>% 
      layer_dense(units = round(ncol*0.8), input_shape = c(ncol), activation = "sigmoid") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = round(ncol*0.15),  activation = "sigmoid") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = round(ncol*0.15), activation ="relu") %>%
      layer_dropout(0.3)  %>%
      layer_dense(units = 1, activation = "sigmoid")
    
    
    model %>% compile(
      loss = focal_loss,
      optimizer = 'adam',
      metrics = f1_m
    )
    
    history <- model %>% fit(
      xtrain,  ytrain, 
      batch_size =0.05,epochs = epoch,
      validation_split = 0.15,
      view_metrics = FALSE
    );
    
    
    #prédiction sur l'échantillon test
    predSimple <- model %>% predict_classes(xtest)
    #print(table(predSimple))
    acc = sum(predSimple == ytest)/length(ytest)
    if(sum(ytest) != 0){
      sensitivity = sum(predSimple == ytest && ytest == 1)/sum(ytest)
    }else{
      sensitivity = 0
    }
    
    sol = cbind(sol, c(i,acc,sensitivity))
    print(c(i,acc,sensitivity))
    
    remove(model, history, predSimple, acc, sensitivity, focal_loss)
  }
  
  return(sol)
  
}








f1 <- function (data, lev = NULL, model = NULL) {
  precision <- Precision(data$pred, data$obs,positive ="1")
  recall  <- Recall(data$pred, data$obs,positive ="1")
  f1_val <- F1_Score(data$pred, data$obs,positive ="1")
  resu=c("precision"=precision,"rappel"=recall,"F1"=f1_val)
  resu
}


```



# Preprocessing

```{r}
importation=function(){
  pb=txtProgressBar(style = 3,width = 50)
  
  setTxtProgressBar(pb, 0.14)
  df = read.csv("DataBase/Speed Dating Data.csv")

  df = df %>% select(-c("positin1","position", "expnum","exphappy"))
  
  #Nettoyage
  setTxtProgressBar(pb, 0.28)
  df = df%>%gestionUndergra()%>%gestionCareer()%>%gestionIncome()

  
  #Homogénisation systeme de points + aggregation
  setTxtProgressBar(pb, 0.42)
  df=agregationVariable(df)
 
  
  # Individus
  setTxtProgressBar(pb, 0.6)
  df_individu = df[!duplicated(df[,c('iid')]),]
  
  listeASupp = c('id','idg','round','wave','order','condtn','int_corr','partner','pid', 'match','samerace','age_o','race_o', 'pf_o_sin','pf_o_int','pf_o_fun','pf_o_amb','pf_o_sha','dec_o', 'sinc_o','attr_o', 'intel_o','fun_o','amb_o','shar_o','like_o','prob_o','met_o','match_es','pf_o_att','dec','attr','sinc','intel','fun','fun','amb','shar','like','prob','met','you_call','them_cal','length', 'date_3', 'numdat_3', 'num_in_3','numdat_2', 'satis_2','positin1','position', 'expnum','attr7_avg','sinc7_avg','intel7_avg','fun7_avg','amb7_avg','shar7_avg','shar3_avg','shar5_avg')
  
  listeASupp=c(listeASupp,c("field","undergra","mn_sat","tuition","from","zipcode","career","income"))
  
  df_individu = df_individu[,-which(names(df) %in% listeASupp)]
  
  
  # Aggregation de variable
  setTxtProgressBar(pb, 0.7)
  df_individu=df_individu%>%aggregateField()%>%aggregateCarrer()%>%aggregateActivitie()
  
  
  # GESTION DES NA
  setTxtProgressBar(pb, 0.8)
  df_individu=gestionDesNASystematiques(df_individu)
  
  # Creation df_couple
  setTxtProgressBar(pb, 0.9)
  df_couple=creationDf_Couple(df,df_individu)
  
  setTxtProgressBar(pb, 1)
  return(df_couple%>%select(-c("pid","iid")))
  
}
```

# Feature engineering

```{r }

feature_eng=function(){
  df_couple2=importation()
  
  # Var numerique -----------------------------------------------------------
  
  resu=creation_avg_ect(df_couple2)
  df_couple2=resu$df
  var_num = resu$var_num
    
  # Var qualitative ---------------------------------------------------------
  
  resu=creation_same_ind(df_couple2)
  df_couple2=resu$df
  var_qual = resu$var_qual

  
  return(list("df"=df_couple2,"var_num"=var_num,"var_qual"=var_qual))
}

```

# Présentation de la base

XXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXX


# Création de la base de donnée finale

feature_eng() permet de prendre notre base de donnée corrigé, transformé, vidé de ces Na, etc... Elle change des formats, mets les variables en numérique et en factor, crée des variables pour diviser les variables factorielles avec beaucoup de modalités en plusieurs variables avec moins de modalités. Elle permet aussi d'enlever les symétrie du modèle (une paire apparait deux fois), elle crée des variables de distances et de moyenne des varaibles numériques, enlève les informations que nous ne pouvons pas savoir à priori (avant le date) etc...

feature_selection() va permettre de donner toutes les varaiables non significatifs à partir d'un seuil. Les variables numériques et factorielles ne sont pas testé de la même façon.

creationDesData() va permettre de créer notre base finale avec aussi les base de test et d'apprentissage.


```{r}
#IMPORTATION JEU DE DONNEE
df_mod=feature_eng()
#var significatif 
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual) # on peut choisir le seuil pour prendre les meilleurs # variables significatifs

# Creation des bases de test et d'apprentissage

resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest
```

## Intelligence artificielle

L'intelligence artificielle est une outif très puissant pour la classification binaire. Nous allons voir si cela est vrai en faisant 4 types de modèles différents. Le premier en changeant la fonction de perte pour donner plus d'importance à la classe 1, le deuxième en mettant des poids plus important à la classe en prenant la fonction de perte standar et les deux dernières avec des méthodes de rééchantillonage : Rose et suréchantillonage.

# Transformation de la base de donnée pour l'intelligence artificielle

Les bases de données doivent transformé en numérique puis en matrix. 
```{r }
dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))


ncol = ncol(dapp) - 1 
xtrain = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain = dapp$match


xtest = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
ytest = dtest$match

#Redéfinition des metric pour l'IA

recall_m=function(y_true, y_pred){
  true_positives = k_sum(k_round(k_clip(y_true * y_pred, 0, 1)))
  possible_positives = k_sum(k_round(k_clip(y_true, 0, 1)))
  recall = true_positives / (possible_positives + k_epsilon())
  return(recall)
} 
precision_m=function(y_true, y_pred){
  true_positives = k_sum(k_round(k_clip(y_true * y_pred, 0, 1)))
  predicted_positives = k_sum(k_round(k_clip(y_pred, 0, 1)))
  precision = true_positives / (predicted_positives + k_epsilon())
  return(precision)
}

# parametre beta a changer
f1_m=function(y_true, y_pred){
  beta = 1
  precision = precision_m(y_true, y_pred)
  recall = recall_m(y_true, y_pred)
  return((1+beta)^2*((precision*recall)/(precision+beta * recall+k_epsilon())))
} 
```

# Intelligence artificielle avec la fonction focal_loss

La fonction de perte focal_loss est connue pour s'occuper de données non équilibrées. En prenant une valeur de gamma grande, on fait en sorte que les bonnes prédiction pour la classe 0 soit moins importantes cependant en augmantant gamma, on diminue fortement la fonction de perte ansi, au commencement même de l'entrainement, on peut avoir une fonction de perte proche de 0 et ainsi, pas efficace. Il faut donc trouver le meilleur gamma capable de donner plsu importance aux bonnes prédictions pour la classe 1 sans avoir une fonciton de perte trop petite et inutilisable.


```{r }
#Optimisation de gamma

# resultat pour gamma
# utiliser cette commande pour optimiser le choix de gamma / cette méthode a ces limites car ne fait une fois l'expérience sur une valeur de gamma et n'est pas en validation croisée. Cela serait trop long et compliqué sinon
#res = optimisationNeuroneDeeplearning(0.2, 10, 0.5, 700, xtrain, xtest,ytrain, ytest);


# loss function et metrics avec choix de gamma

choix_gamma = 5

focal_loss=function(y_true, y_pred){
  gamma = choix_gamma
  
  print("gamma")
  pt = y_pred * y_true + (1-y_pred) * (1-y_true)
  print("pt")
  pt = k_clip(pt, 0, 1)
  print("pt2")
  CE = -k_log(pt+k_epsilon())
  print("CE")
  FL = k_pow(1-pt, gamma) * CE
  print("FL")
  loss = k_sum(FL, axis=1)
  print(loss)
  return(loss)
}


# creation du modèle / on prend ici un modèle simple mais tout de meme assez compliqué pour résoudre cette tache

model <- keras_model_sequential()
model %>% 
  layer_dense(units = round(ncol*0.8), input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15), activation ="relu") %>%
  layer_dropout(0.3)  %>%
  
  layer_dense(units = 1, activation = "sigmoid")




# compilation, apprentissage et prédiction

model %>% compile(
  loss = focal_loss, # utilisation de cette nouvelle foncton de perte
  optimizer = 'adam',
  metrics = f1_m # metrique utilisé : f1 score
)

history <- model %>% fit(
  xtrain,  ytrain, 
  
  batch_size =0.05,epochs = 200,
  validation_split = 0.2
)


#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))

```

Les résultats sont assez mauvais, le problème étant que le modèle est très instable à cause de la fonction de perte et même en changeant le gamma, on obtient encore des résultats médiocres. Passons donc à une autre faon de modéliser ce problème.

On va ici mettre des poids aux classes tout en utilisant la fonction de perte usuelles pour les classifieurs binaires soit la "binary crossentropy". La valeur de poids à mettre à la classe i est juste le nombre d'indivus de la classs majoritaire divisé par le nombre d'individus de classe i. On obtient ici pour la classe 0 : 1 et pour la classe 1 : 6.0284857571. Mettons en place ce nouveau modèle :

```{r}
# poids


model <- keras_model_sequential()
model %>% 
  layer_dense(units = round(ncol*0.8), input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15),  activation = "sigmoid") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.15), activation ="relu") %>%
  layer_dropout(0.3)  %>%
  
  layer_dense(units = 1, activation = "sigmoid")



model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
  
)
history <- model %>% fit(
  xtrain,  ytrain, 
  
  batch_size =0.05,epochs = 200,
  validation_split = 0.2,
  class_weight=list("0"=1,"1"=6.0284857571)
)


#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))
```
On obtient de bien meilleurs résulats, on peut changer le nombre d'epoch pour essayer d'obtenir encore de meilleur résultats. epochs à 200 n'est pas le meilleur paramètre, il est juste à cette valeur pour éviter d'avoir trop à attendre.Voici le meilleur modèle de poids :


```{r}

model_poids = load_model_hdf5("Modelisation/IA/model_f1_poids_35806_2.hdf5")

l = seq(0.1,0.9,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_poids,xtrain)>s), ytrain, positive = "1"))
f1_list

meilleur_seuil_poids = l[which.max(f1_list)]
f1_poids = f1_list[which.max(f1_list)]


predSimple = as.integer(predict_proba(model_poids,xtest)>meilleur_seuil_poids)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest),positive="1",mode = "prec_recall"))
```

# Intelligence artificielle avec méthode de Sampling : Rose

Passons maintenant aux différentes méthode d'échantillonange permetttant de ré équilibré les classes et commencant par la méthode d'échantillonage ROSE. Cette méthode crée une base de donnée équilibré par des méthodes de bootstrap et d'individus artificielles. Elle donne souvent de bons résulats.

Création des différentes bases nécessaires

On change de base car il faut moins de variables pour le nombre d'individus pour que les méthodes de rééchantillonage fonctionne.

```{r}
df_mod=feature_eng()
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual,0.1) # on change ici le seuil car Rose a besoin #d'avoir assez d'individus pour le nombre de variables donc on diminue le nombre de varaibles 
#Split
resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest

#base normale

dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))

ncol = ncol(dapp) - 1 
xtrain_normal = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_normal = dapp$match


xtest_normal = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
ytest_normal = dtest$match

# base rose

df_mod <- ROSE(match ~ ., data = df_mod, seed = 1)$data
resu=creationDesData(df_mod,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest

dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))


ncol = ncol(dapp) - 1 
xtrain_rose = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_rose = dapp$match
Xtest_rose = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
Ytest_rose = dtest$match
```

Creation et mise en route du modèle

```{r}
# creation du modèle

model <- keras_model_sequential()
model %>% 
  layer_dense(units = ncol, input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dense(units = ncol,  activation = "relu") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.5), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.5), activation ="relu") %>%
  layer_dropout(0.2)  %>%
  layer_dense(units = round(ncol*0.3),  activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.3), activation ="relu") %>%
  layer_dropout(0.1)  %>%
  layer_dense(units = 1, activation = "sigmoid")



# compilation, apprentissage et prédiction

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',#optimizer_rmsprop()
  metrics = 'accuracy'
)
history <- model %>% fit(
  xtrain_rose,  ytrain_rose, 
  batch_size =0.1,epochs = 100,
  validation_split = 0.1
)
#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest_normal)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
```

On obtient de très bons modèles avec cette méthode ci et c'est plutôt facile à implémenter.

Meilleur modèle avec Rose trouvée :

```{r}
# méthode Rose

model_rose = load_model_hdf5("Modelisation/IA/model_f1_rose_41365_2.hdf5")

l = seq(0.05,0.95,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_rose,xtrain_normal)>s), ytrain_normal, positive = "1"))
f1_list

meilleur_seuil_rose = l[which.max(f1_list)]
f1_rose = f1_list[which.max(f1_list)]

predSimple = as.integer(predict_proba(model_rose,xtest_normal)>meilleur_seuil_rose)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
```


# Intelligence artificielle avec méthode d'échantillonage : suréchantillonage

Pour les mêmes raisons, on change de base.

Création des différentes bases nécessaires
```{r}
df_mod=feature_eng()
var_signif=feature_selection(df_mod$df,df_mod$var_num,df_mod$var_qual,0.1)
#Split
resu=creationDesData(df_mod$df,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest


# base oversampling

df_mod <- ovun.sample(match ~ ., data = df_mod, method = "over",N = 2*(nrow(df_mod) - sum(as.numeric(df_mod$match)-1)))$data

table(df_mod$match)


resu=creationDesData(df_mod,var_signif)
df_mod = resu$df_mod;train_data = resu$dapp;test_data = resu$dtest

dapp = data.frame(lapply(train_data, function(x) as.numeric(as.character(x))))
dtest = data.frame(lapply(test_data, function(x) as.numeric(as.character(x))))

ncol = ncol(dapp) - 1 
xtrain_over = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain_over = dapp$match
Xtest_over = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
Ytest_over = dtest$match
```



# creation du modèle et mise en route
```{r}
model <- keras_model_sequential()
model %>% 
  layer_dense(units = ncol, input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dense(units = ncol,  activation = "relu") %>%
  layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.5), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.5), activation ="relu") %>%
  layer_dropout(0.2)  %>%
  layer_dense(units = round(ncol*0.3),  activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.3), activation ="relu") %>%
  layer_dropout(0.1)  %>%
  layer_dense(units = 1, activation = "sigmoid")



# compilation, apprentissage et prédiction

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)
history <- model %>% fit(
  xtrain_over,  ytrain_over, 
  batch_size =0.1,epochs = 100,
  validation_split = 0.1
)
#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(xtest_normal)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))


print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```

Les résultats sont excellents avec cette méthode même avec des epochs faibles. APrès à partir de 1000 epochs, les résultats sont moins bons donc il faut faire attention.

Meilleurs modèles avec la méthode de suréchantillonage :


Voiciun modèle trouvée à 200 epochs, il va permettre de complémenter le modèle à 700 epochs pour la suite du code
```{r}
model_over_1 = load_model_hdf5("Modelisation/IA/model_f1_over_5008_2.hdf5")

l = seq(0.05,0.9,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_over_1,xtrain_normal)>s), ytrain_normal, positive = "1"))
f1_list

meilleur_seuil_over_1 = l[which.max(f1_list)]
f1_over_1 = f1_list[which.max(f1_list)]

print("prediction avec le meilleur seuil de f1_score")
predSimple = as.integer(predict_proba(model_over_1,xtest_normal)>meilleur_seuil_over_1)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))

print("prédiction avec le seuil standard de 0.5")
predSimple = as.integer(predict_proba(model_over_1,xtest_normal)>0.5)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))

```

Meilleur modèle tout confondu trouvée avec un F1-score de 0.5737 avec le meilleur score trouvé pour maximiser le F1-score sur la base d'entrainement et un F1-score de 0.5752 avec le seuil standard de 0.5. Le nombre d'epochs est de 700.

```{r}
model_over_2 = load_model_hdf5("Modelisation/IA/model_f1_over_5752_2.hdf5")

l = seq(0.05,0.9,0.01)
f1_list = sapply(l, function(s) F1_Score(as.integer(predict_proba(model_over_2,xtrain_normal)>s), ytrain_normal, positive = "1"))
f1_list

meilleur_seuil_over_2 = l[which.max(f1_list)]
f1_over_2 = f1_list[which.max(f1_list)]

print("prediction avec le meilleur seuil de f1_score")
predSimple = as.integer(predict_proba(model_over_2,xtest_normal)>meilleur_seuil_over_2)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))

print("prédiction avec le seuil standard de 0.5")
predSimple = as.integer(predict_proba(model_over_2,xtest_normal)>0.5)
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))

```
# Combinaison de modèles IA

Nous avons vu que les modèles les plus performants utilisés une méthode de ré échantillonage pour contre balancer le déséquilibre des classes avec tout de même mentions honorables à l'IA qui se sert de poids sur les classes sans rééchantillonage. Nous allons maintenant essayer de combiner ces modèles afin que les avantages des uns rattrapent les inconvénients des autres. Pour cela, nous allons effectué des vote à la majorité ou sommer les probabilités données par les IA ou même pondéré ces résultats par rapport aux meilleurs F1_score qu'ils trouvent sur la base d'apprentissage. Le modèle avec le poids ne sera pas utilisé car trop mauvais comparé aux autres. Nous allons au plus combiné les 2 IA de suréchantillonage et l'IA d'échantillonage Rose.


Ici un vote à la majorité avec les meilleurs IA : 2 suréchantillonage et 1 Rose. On obtient un F1_score de 0.5732 ce qui est sensiblement pareil que le F1_score de "model_over_2" seul.
```{r}
predSimple = as.integer(as.integer(predict_proba(model_rose,xtest_normal)>0.5)
                        +as.integer(predict_proba(model_over_1,xtest_normal)>0.5)
                        +as.integer(predict_proba(model_over_2,xtest_normal)>0.5)
                        >1.5)


print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```

On fait ce même vote à la majorité avec les 3 mêmes IA mais les différents meilleurs seuils qui maximisent le F1_score que nous avons trouvé précédemmen.On trouve ici un F1_score de 0.5928 ce qui est le meilleur modèle pour l'instant trouvé.

```{r}
predSimple = as.integer(as.integer(predict_proba(model_rose,xtest_normal)>meilleur_seuil_rose)
                        +as.integer(predict_proba(model_over_1,xtest_normal)>meilleur_seuil_over_1)
                        +as.integer(predict_proba(model_over_2,xtest_normal)>meilleur_seuil_over_2)
                        >1.5)


print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```
Maintenant, on utilise les probabilité données par les IA en les pondérant en fonction de leur F1_score respective permettant de donner plus de poids au meilleur modèle. Le F1_score est de 0.58 ce qui n'est pas incroyable.

```{r}
# avec pondération sur les probas : 3 modèles

predSimple = as.integer(f1_rose*predict_proba(model_rose,xtest_normal)
                        + f1_over_1*predict_proba(model_over_1,xtest_normal)
                        + f1_over_2*predict_proba(model_over_2,xtest_normal)
                                   >(f1_rose+f1_over_1+f1_over_2)/2)

print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```

On refait maintenant le même travail mais avec juste que les deux IA de suréchantillonage (les deux meilleurs et qui se complémentent bien en fonction de leur précision et sensitivité). On trouve un modèle incroyable en faisant un vote à la majorité avec un F1_score de 0.6065, une accuracy d'environ 87% et une sensitivité forte permettant de trouver des bons matchs.
```{r}
predSimple = as.integer(as.integer(predict_proba(model_over_1,xtest_normal)>meilleur_seuil_over_1)
                        +as.integer(predict_proba(model_over_2,xtest_normal)>meilleur_seuil_over_2)
                        >1)


print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```
Ici, on utilise la pondération des probabilités comme vu précédemment mais avec que les deux modèles de suréchantillonage. On trouve un modèle avec un meilleur F1_score avec 0.6091 cependant l'accuracy est moins bonne ce qui est fait peut-être de ce modèle un moins bonne modèle.


```{r}
#  : 2 modèles : f1_score de 0.6091 >> meilleur modele a ce jour mais moins bonne précision

predSimple = as.integer(f1_over_1*predict_proba(model_over_1,xtest_normal)
                        + f1_over_2*predict_proba(model_over_2,xtest_normal)
                        >(f1_over_1+f1_over_2)/2)

print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1",mode = "prec_recall"))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(ytest_normal),positive="1"))
```
