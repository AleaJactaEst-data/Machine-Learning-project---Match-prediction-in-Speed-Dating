---
title: "Dating_data : la data qui vous date"
author: "Le super groupe de travaille fada de MACHINE LEARNINNNNNNG"
date: "26/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("dplyr")
library("stringr")
library("caret")
library(utils)
library(e1071)
library(ggplot2)
library(ROSE)

library("keras")
library("tensorflow")
```



# Base de donnée et Ouverture du dataset 


```{r}
df = read.csv("../DataBase/Speed Dating Data.csv")
df
dim(df)
```


# Suppression colonne inutile 

```{r}
#Reste a supprimer : career, (apres nettoyage)
df = df %>% dplyr::select(-c("positin1","position", "expnum","exphappy"))
```

# Nettoyage du dataset

```{r}
#Gestion des non diplomés
df = gestionUndergra(df)

#Gestion des indices metiers
df = gestionCareer(df)

#gestion de Income (numeric)
df = gestionIncome(df)
```


#Gestion des valeurs manquantes 

(1800): attr4_1...shar4_2 : doit-on les supprimer ou les remplaces ?
#Relation lineaire car la somme vaut 100

(3500): attr5_1...shar5_2 : doit-on les supprimer ou les remplaces ?
#Relation lineaire car la somme vaut 100

attr1_s ... amb1_s : 4400 NA values , attr7_2 ... amb7_2 : 6394 NA values, attr4_2 ... amb2_2 : 2603 NA values
attr2_2 ... amb2_2 : 2603 NA values, attr5_2 ... amb5_2 : 4001 NA values
numdat_3 : 6882, num_in_3 : 7710, attr7_3 ... amb7_3 : 6382, fun4_3 ... : 5419


Solution regrouper les variables


# agreger les variables
```{r} 
df2 = agregationVariable(df)

length(df2$fun5_avg[which(df2$match == 1 & is.na(df2$sinc5_avg)) ])/length(df2$fun5_avg[which(df2$match == 1) ])
length(df2$fun5_avg[which(df2$match == 0 & is.na(df2$sinc5_avg)) ])/length(df2$fun5_avg[which(df2$match == 0) ])
```


# creation de df_individu
```{r} 
df_individu = df2[!duplicated(df2[,c('iid')]),]

listeASupp = c('id','idg','round','wave','order','condtn','int_corr','partner','pid', 'match','samerace','match_es','dec',
'age_o','race_o', 'pf_o_sin','pf_o_int','pf_o_fun','pf_o_amb','pf_o_sha','dec_o', 'sinc_o','attr_o', 'intel_o','fun_o','amb_o','shar_o','like_o','prob_o','met_o','pf_o_att',
'attr','sinc','intel','fun','amb','shar','like','prob','met',
'you_call','them_cal','length', 'date_3', 'numdat_3', 'num_in_3','numdat_2', 'satis_2',
'attr7_avg','sinc7_avg','intel7_avg','fun7_avg','amb7_avg','shar7_avg','shar3_avg','shar5_avg',
"field","mn_sat","tuition","from","zipcode","career","income")



df_individu = df_individu %>% dplyr::select(-all_of(listeASupp))
```

# gestion des NA \ remplacement par la moyenne et supression d'individu

```{r} 
df_individu = gestionDesNASystematiques(df_individu)

for(var in c("age","field_cd", "date", "amb2_avg", "shar2_avg")){
  id = which(is.na(df_individu[,var]))
    if(length(id)>0){
      df_individu = df_individu[-id,]
    }
}
```

# creation de la base pour merge

```{r}
df_couple_id = creationDf_Couple_Id(df2)
```

#creation de df_couple en mergeant

```{r}
df_couple = merge(merge(df_couple_id, df_individu, by = 'iid'), df_individu, by.x = 'pid', by.y = 'iid')
```

# visualisation des différents DataSet

```{r}
df2
df_individu
df_couple_id
df_couple
#na de individus
var_na=sapply(names(df_individu),function(x) sum(is.na(df_individu[,x])))
barplot(var_na[var_na>0]) # bug car il n'y a plus de NA

```

# ajout en plus des val numérique .x et . y  des moyennes et ecarts

```{r}
res = creation_avg_ect(df_couple)
df_couple2 = res$df
varNum = res$var_num
varFactor = c(res$var_qual, "match")
remove(res)
```

# creation d'une variable distance (moyenne des écarts)

```{r}
liste_ect =c("imprace_ect","imprelig_ect","sports_ect","tvsports_ect","exercise_ect","dining_ect","museums_ect","art_ect","hiking_ect","gaming_ect",
"clubbing_ect","reading_ect","tv","theater_ect","movies_ect","concerts_ect","music_ect","shopping_ect","yoga_ect",
          paste0("attr",1:5,"_avg_ect"),paste0("sinc",1:5,"_avg_ect"),paste0("intel",1:5,"_avg_ect"),paste0("fun",1:5,"_avg_ect"),paste0("amb",1:5,"_avg_ect"),paste0("shar",c(1,2,4),"_avg_ect"))

j = sapply(1:nrow(df_couple2), function(i) sqrt(sum(df_couple2[i,which(names(df_couple2) %in% liste_ect)]^2)))


df_couple2 <- df_couple2 %>%
    mutate(distance = j)

remove(mod, v, v_modalite,  j, liste_ect, var_na, id, listeASupp, var)
```


# normalisation des donnees
```{r}

df_couple2$match = as.factor(df_couple2$match)

for(var in names(df_couple2)){
  if(!is.factor(df_couple2[1,var])){
    df_couple2[,var] = (df_couple2[,var]-mean(df_couple2[,var]))/sd(df_couple2[,var])
    
  }
}
df_couple2 = df_couple2[,-which(names(df_couple2) %in% c("gender.x", "gender.y", "go_out_6", "id", "pid"))]

remove(var)
```


# Visualisation des densités
```{r}
var = df_couple2$distance
ggplot(df_couple2, aes(x=var, color=factor(match))) + geom_density()
```
# variable significatif RandomForest
```{r}
rf_mod = randomForest::randomForest(match~., data = df_couple2)

id = order(rf_mod$importance)
temp = rf_mod$importance
bar = cbind.data.frame(rownames(temp),temp )

colnames(bar) = c("Var", "MeanDecreaseGini")

library(ggplot2)
# Barplot basique
p<-ggplot(data=bar, aes(x=Var, y=MeanDecreaseGini)) +
  geom_bar(stat="identity")

# Barplot horizontal
p + scale_x_discrete(limits=rownames(temp)[id]) + coord_flip()



varRandom_1 = rownames(temp)[which(temp > mean(temp))]
varRandom_2 = rownames(temp)[which(temp > mean(temp) + sd(temp))]
```

# variable significatif Boosting
```{r}

mod_boost = gbm(match~.,data=df_couple2,distribution="adaboost",interaction.depth=2,shrinkage=0.1,n.trees=500)
res = summary(mod_boost)

varBoost_1 = as.character(res$var[which(res$rel.inf > mean(res$rel.inf))])
varBoost_2 = as.character(res$var[which(res$rel.inf > mean(res$rel.inf) + sd(res$rel.inf))])
```
# Création des échantillons test et d'apprentissage avec les var significatifs > df_mod
```{r}

varXGBoost = c( "same_career_c", "date_2"      ,     "career_c_3"    ,  "career_c_10",   "go_out_2"   ,  "career_c_7"    ,   "go_out_4"   ,   "career_c_2" ,  "career_c_12"  ,
 "date_4"       ,     "date_5"       ,   "field_cd_10"  ,   "career_c_6"   ,
 "field_cd_16" ,  "goal.x"         ,   "race_2"   ,        
 "date.x"     ,      
 "field_cd_15" ,  "go_out_5" ,       "movies_moy"  ,   "shar4_avg_ect" , "clubbing_ect"  , "music_moy"   ,   "hiking_moy" ,   
 "sinc2_avg.y"  ,  "exercise.y"   ,  "fun4_avg.y"  ,   "amb3_avg_ect"  , "intel4_avg_ect" ,"theater.x" ,     "fun2_avg.x"   ,  "dining.x"   ,   
 "sinc2_avg_ect" , "tv_ect"      ,   "reading_moy"  ,  "theater.y"    ,  "attr1_avg.y"  ,  "hiking.x" ,      "intel3_avg.y" , "match", "distance")


varSupp = c("gender.x", "gender.y","id", "pid", "go_out_6", "same_gender")

# si on veut pas renseigner varSignificatifs, varSupp >> ne rien mettre
res = creationDesData(df_couple2, varSignificatifs = varXGBoost, varSupp =  varSupp, noFactor = FALSE)
df_mod = res$df_mod
dapp = res$dapp
dtest = res$dtest
```

# Modèle Logistique

# permet de trouver des bonnes var significatifs
```{r}
#FulMod <- glm(match~.,data=dapp,family=binomial(link=logit))
#step(FulMod,direction="backward",trace=TRUE)
```

# Essai du modèle

```{r}
best_mod_real = glm(formula = match ~ ., family = binomial(link = logit), data = dapp) 

pred_best = predict(best_mod_real, newdata = dtest, type = "response")

# Le but est de prendre les 16% meilleur score est de mettre leur valeur à 1
# pour dire qu'il y a match
pred_best[order(pred_best)][0:round(0.80*nrow(dtest))]=0 # je mets à 0 les plus petits
pred_best[order(pred_best)][(round(0.80*nrow(dtest))+1):nrow(dtest)]=1 # je mets à 1 les plus grands

print(caret::confusionMatrix(data=factor(pred_best),reference=dtest$match,positive="1"))
```

# TEST SVM


```{r}

# on teste differents paramètres pour mieux les choisir (c'est long)
"""
tune.svm_ <- tune(
  svm, match~.,
  data = dapp, 
  ranges = list( cost = seq(68.1,69,0.2), gamma = seq(0.017,0.025,0.002), kernel = c("radial")),
  tunecontrol = tune.control(sampling = "fix"),
  validation.x = dtest[,-1],
  validation.y = dtest$match
)

tune.svm_
"""
# meilleur param pour radial > cost = 69 (0.15) > gamma = 0.02
```

# test tout seul d'un svm


```{r} 


mod.svm <- svm(match~.,data=dapp,kernel="radial", cost=1000000000000000000000000000000000000000000000)
#mod.svm <- svm(match~.,data=dapp,kernel="radial", cost=100)
#mod.svm <- svm(match~.,data=dapp,kernel="polynomial", cost=100)
#mod.svm <- svm(match~.,data=dapp,kernel="linear", cost=100)


# resultat sur le modèle svm tester
pred.svm = predict(mod.svm, newdata = dtest)
print(caret::confusionMatrix(data=pred.svm,reference=dtest$match,positive="1"))

```


# Intelligence artificielle

# Creation des bases de test et d'apprentissage

```{r}
#df_mod = df_couple2[,which(names(df_couple2) %in% varSignificatifs)]
#0df_mod = df_couple2[,which(names(df_couple2) %in% c("age_ect", "same_race", "same_goal", "same_field_cd", "distance", "imprace_moy", "imprelig_moy", "sport_moy","intel1_avg_moy" , "fun1_avg_moy", "match"))]

#df_mod =df_couple2[,-which(names(df_couple2) %in% varListe)]
# utilisation de rose pour de meilleur résultats
res = creationDesData(df_couple2, varSignificatifs = varXGBoost, varSupp =  varSupp, noFactor = FALSE)
df_mod = res$df_mod

res = creationDesData(df_mod, noFactor = TRUE)
df_mod = res$df_mod
dapp = res$dapp
dtest = res$dtest


ncol = ncol(dapp) - 1 

xtrain = as.matrix(dapp[, -which(names(dapp) %in% c("match"))])
ytrain = dapp$match

Xtest = as.matrix(dtest[, -which(names(dtest) %in% c("match"))])
Ytest = dtest$match
```

# loss function et metrics

```{r}

recall_m=function(y_true, y_pred){
  true_positives = k_sum(k_round(k_clip(y_true * y_pred, 0, 1)))
  possible_positives = k_sum(k_round(k_clip(y_true, 0, 1)))
  recall = true_positives / (possible_positives + k_epsilon())
  return(recall)
} 
precision_m=function(y_true, y_pred){
  true_positives = k_sum(k_round(k_clip(y_true * y_pred, 0, 1)))
  predicted_positives = k_sum(k_round(k_clip(y_pred, 0, 1)))
  precision = true_positives / (predicted_positives + k_epsilon())
  return(precision)
}

# parametre beta a changer
f1_m=function(y_true, y_pred){
  beta = 1
  precision = precision_m(y_true, y_pred)
  recall = recall_m(y_true, y_pred)
  return((1+beta)^2*((precision*recall)/(precision+beta * recall+k_epsilon())))
} 

focal_loss=function(y_true, y_pred){
  gamma = 5.3 # 3.8 a 7.5 fonctionne > test - amelioration surechantilonage
  print("gamma")
  pt = y_pred * y_true + (1-y_pred) * (1-y_true)
  print("pt")
  pt = k_clip(pt, 0, 1)
  print("pt2")
  CE = -k_log(pt+k_epsilon())
  print("CE")
  FL = k_pow(1-pt, gamma) * CE
  print("FL")
  loss = k_sum(FL, axis=1)
  print(loss)
  return(loss)
}

```


# creation du modèle
```{r}
model <- keras_model_sequential()
model %>% 
  layer_dense(units = ncol, input_shape = c(ncol), activation = "sigmoid") %>%
  layer_dense(units = ncol,  activation = "relu") %>%
layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.9), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.9), activation = "relu") %>%
layer_dropout(0.3)  %>%
  layer_dense(units = round(ncol*0.8), activation = "relu") %>%
layer_dropout(0.25)  %>%
  layer_dense(units = round(ncol*0.7), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.7), activation = "relu") %>%
  
layer_dropout(0.25)  %>%
  layer_dense(units = round(ncol*0.5), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.5), activation ="relu") %>%
layer_dropout(0.25)  %>%
  layer_dense(units = round(ncol*0.5), activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.5), activation ="relu") %>%
layer_dropout(0.25)  %>%
  layer_dense(units = round(ncol*0.3),  activation = "sigmoid") %>%
  layer_dense(units = round(ncol*0.5), activation ="relu") %>%
layer_dropout(0.15)  %>%
  layer_dense(units = 1, activation = "sigmoid")
```


# compilation, apprentissage et prédiction
```{r}
model %>% compile(
  loss = focal_loss,
  optimizer = 'adam',#optimizer_rmsprop()
  metrics = f1_m
)

history <- model %>% fit(
  xtrain,  ytrain, 
  batch_size =0.1,epochs = 2000,
  validation_split = 0.2
)


#prédiction sur l'échantillon test
predSimple <- model %>% predict_classes(Xtest)
#print(table(predSimple))
print(caret::confusionMatrix(data=factor(predSimple),reference=factor(Ytest),positive="1"))
```



Différentes lignes de commande pour les modeles de Machine Learning

```{r}
# Print a summary of a model
summary(model)

# Get model configuration
get_config(model)

# Get layer configuration
get_layer(model, index = 1)

# List the model's layers
model$layers

# List the input tensors
model$inputs

# List the output tensors
model$outputs
```



```{r}
f1 <- function (data, lev = NULL, model = NULL) {
  experience<<-data
  precision <- Precision(data$pred, data$obs,positive ="1")
  recall  <- Recall(data$pred, data$obs,positive ="1")
  f1_val <- F1_Score(data$pred, data$obs,positive ="1")
  resu=c("precision"=precision,"rappel"=recall,"F1"=f1_val)
  print(resu)
  resu
}
```
